{
  "execution_id": "916ce108-cb1c-4594-b9ec-2e73ca140f9f",
  "status": "completed",
  "started_at": "2025-07-21T13:34:09.478842+00:00",
  "updated_at": "2025-07-21T13:34:19.667905+00:00",
  "query": "What are the key architectural improvements in modern transformer models for handling long sequences?",
  "thinking_mode": "intermediate",
  "total_permutations": 720,
  "completed_permutations": 720,
  "current_sequence": "",
  "estimated_completion": null,
  "permutations_per_minute": 0.0,
  "elapsed_hours": 0.0005228577719794379,
  "estimated_hours_remaining": 0.0,
  "memory_usage_mb": 0.0,
  "cpu_usage_percent": 0.0,
  "ftns_tokens_distributed": 0.0,
  "ftns_payments_count": 0,
  "confidence_score": 0.0,
  "papers_analyzed": 0,
  "final_answer_length": 0,
  "citations_count": 0,
  "error_count": 0,
  "last_error": "",
  "recovery_count": 0
}