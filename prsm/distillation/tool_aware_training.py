"""
Tool-Aware Training Pipeline for PRSM
Enhanced knowledge distillation with MCP tool use capabilities

This module extends the production training pipeline to include tool-aware training,
enabling distilled models to learn when and how to use MCP tools effectively.
The training system teaches models to:
- Recognize when tools are needed for a task
- Select appropriate tools from available options
- Use tools correctly with proper parameters
- Coordinate multiple tools for complex workflows
- Handle tool failures and fallback strategies

Architecture Integration:
- Extends existing production training pipeline
- Integrates with MCP Tool Router and Tool Marketplace
- Uses tool execution sandbox for safe training
- Connects to FTNS service for cost tracking
- Provides comprehensive analytics and monitoring
"""

import asyncio
import json
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from uuid import UUID, uuid4
from dataclasses import dataclass, field
from pathlib import Path
from enum import Enum

import structlog
from pydantic import BaseModel, Field

from ..core.config import get_settings
from ..core.models import TeacherModel, ModelType, TimestampMixin
from ..agents.routers.tool_router import ToolRouter, ToolRequest, MCPToolSpec, ToolType, ToolCapability
from ..marketplace.legacy.tool_marketplace import tool_marketplace, ToolListing
from ..integrations.security.sandbox_manager import SandboxManager, ToolExecutionRequest
from ..tokenomics.ftns_service import FTNSService
from .production_training_pipeline import ProductionTrainingPipeline, TeacherModelConnector, TrainingConfig

logger = structlog.get_logger(__name__)
settings = get_settings()


class ToolTrainingStrategy(str, Enum):
    """Strategies for tool-aware training"""
    BASIC_AWARENESS = "basic_awareness"          # Learn to recognize when tools are needed
    TOOL_SELECTION = "tool_selection"            # Learn to select appropriate tools
    TOOL_COORDINATION = "tool_coordination"      # Learn to coordinate multiple tools
    WORKFLOW_OPTIMIZATION = "workflow_optimization"  # Learn to optimize tool workflows
    SAFETY_PROTOCOLS = "safety_protocols"       # Learn safety and error handling


class ToolUsagePattern(str, Enum):
    """Common tool usage patterns for training"""
    SINGLE_TOOL = "single_tool"                  # Use one tool for a task
    SEQUENTIAL = "sequential"                    # Use tools in sequence
    PARALLEL = "parallel"                       # Use tools in parallel
    CONDITIONAL = "conditional"                  # Use tools based on conditions
    RECURSIVE = "recursive"                      # Tools that call other tools


@dataclass
class ToolTrainingExample:
    """Training example with tool usage"""
    task_description: str
    input_data: Dict[str, Any]
    expected_tools: List[str]
    tool_sequence: List[Dict[str, Any]]
    expected_output: str
    success_criteria: Dict[str, Any]
    difficulty_level: float = 0.5
    tool_pattern: ToolUsagePattern = ToolUsagePattern.SINGLE_TOOL
    safety_requirements: List[str] = field(default_factory=list)


class ToolAwareDataset(Dataset):
    """Dataset for tool-aware training with real tool interactions"""
    
    def __init__(self, tool_router: ToolRouter, sandbox_manager: SandboxManager,
                 domain: str, training_strategy: ToolTrainingStrategy,
                 size: int = 1000, tokenizer=None):
        self.tool_router = tool_router
        self.sandbox_manager = sandbox_manager
        self.domain = domain
        self.training_strategy = training_strategy
        self.tokenizer = tokenizer
        self.size = size
        
        # Training examples
        self.examples: List[ToolTrainingExample] = []
        self.tool_usage_cache: Dict[str, Any] = {}
        
        # Available tools for this domain
        self.available_tools = []
        
        logger.info("Initializing tool-aware dataset",
                   domain=domain,
                   strategy=training_strategy.value,
                   size=size)
    
    async def initialize(self):
        """Initialize dataset with tool discovery and example generation"""
        # Discover available tools for domain
        await self._discover_domain_tools()
        
        # Generate training examples
        await self._generate_training_examples()
        
        # Execute tool interactions for ground truth
        await self._execute_tool_interactions()
        
        logger.info("Tool-aware dataset initialized",
                   examples=len(self.examples),
                   tools=len(self.available_tools))
    
    async def _discover_domain_tools(self):
        """Discover tools relevant to the training domain"""
        try:
            # Search marketplace for domain-relevant tools
            search_result = await tool_marketplace.search_tools(
                query=self.domain,
                limit=20
            )
            
            self.available_tools = []
            for tool_data in search_result["tools"]:
                tool_listing = tool_marketplace.listings.get(tool_data["tool_id"])
                if tool_listing:
                    self.available_tools.append(tool_listing)
            
            # Add built-in tools from tool router
            for tool_id, tool_spec in self.tool_router.tool_registry.tools.items():
                if self._is_tool_relevant(tool_spec, self.domain):
                    # Create a mock listing for built-in tools
                    mock_listing = type('MockListing', (), {
                        'tool_spec': tool_spec,
                        'pricing_model': 'free',
                        'quality_grade': 'verified'
                    })()
                    self.available_tools.append(mock_listing)
            
            logger.info("Discovered domain tools",
                       domain=self.domain,
                       tool_count=len(self.available_tools))
        
        except Exception as e:
            logger.error("Tool discovery failed", error=str(e))
            self.available_tools = []
    
    def _is_tool_relevant(self, tool_spec: MCPToolSpec, domain: str) -> bool:
        """Check if a tool is relevant to the training domain"""
        domain_keywords = {
            "research": ["search", "web", "data", "analysis"],
            "coding": ["python", "code", "execute", "debug"],
            "data_analysis": ["data", "statistics", "visualization", "compute"],
            "communication": ["api", "request", "send", "notify"],
            "file_management": ["file", "read", "write", "storage"]
        }
        
        keywords = domain_keywords.get(domain.lower(), [])
        tool_text = f"{tool_spec.name} {tool_spec.description}".lower()
        
        return any(keyword in tool_text for keyword in keywords)
    
    async def _generate_training_examples(self):
        """Generate training examples based on strategy and available tools"""
        example_generators = {
            ToolTrainingStrategy.BASIC_AWARENESS: self._generate_awareness_examples,
            ToolTrainingStrategy.TOOL_SELECTION: self._generate_selection_examples,
            ToolTrainingStrategy.TOOL_COORDINATION: self._generate_coordination_examples,
            ToolTrainingStrategy.WORKFLOW_OPTIMIZATION: self._generate_optimization_examples,
            ToolTrainingStrategy.SAFETY_PROTOCOLS: self._generate_safety_examples
        }
        
        generator = example_generators.get(
            self.training_strategy, 
            self._generate_awareness_examples
        )
        
        await generator()
        
        logger.info("Generated training examples",
                   strategy=self.training_strategy.value,
                   examples=len(self.examples))
    
    async def _generate_awareness_examples(self):
        """Generate examples for basic tool awareness training"""
        templates = [
            {
                "task": "Find recent information about {topic}",
                "tools": ["web_search"],
                "pattern": ToolUsagePattern.SINGLE_TOOL,
                "topics": ["machine learning", "climate change", "quantum computing", "biotechnology"]
            },
            {
                "task": "Calculate the result of {expression}",
                "tools": ["python_executor"],
                "pattern": ToolUsagePattern.SINGLE_TOOL,
                "topics": ["2 + 2", "sqrt(144)", "factorial(5)", "prime_factors(100)"]
            },
            {
                "task": "Read and summarize the file {filename}",
                "tools": ["file_reader"],
                "pattern": ToolUsagePattern.SINGLE_TOOL,
                "topics": ["research_paper.pdf", "data_analysis.csv", "project_notes.txt", "meeting_transcript.docx"]
            },
            {
                "task": "Query the database for {query}",
                "tools": ["database_query"],
                "pattern": ToolUsagePattern.SINGLE_TOOL,
                "topics": ["user statistics", "sales data", "inventory levels", "performance metrics"]
            }
        ]
        
        for template in templates:
            for topic in template["topics"]:
                task_description = template["task"].format(
                    topic=topic, 
                    expression=topic, 
                    filename=topic, 
                    query=topic
                )
                
                example = ToolTrainingExample(
                    task_description=task_description,
                    input_data={"topic": topic},
                    expected_tools=template["tools"],
                    tool_sequence=[{
                        "tool_id": template["tools"][0],
                        "action": "execute",
                        "parameters": {"query": topic}
                    }],
                    expected_output=f"Using {template['tools'][0]} to handle: {task_description}",
                    success_criteria={"tool_selected": True, "correct_tool": True},
                    tool_pattern=template["pattern"]
                )
                
                self.examples.append(example)
    
    async def _generate_selection_examples(self):
        """Generate examples for tool selection training"""
        selection_scenarios = [
            {
                "task": "I need to analyze this dataset and create visualizations",
                "available_tools": ["python_executor", "data_visualizer", "file_reader"],
                "optimal_selection": ["file_reader", "python_executor", "data_visualizer"],
                "reasoning": "Read data, analyze with Python, then visualize results"
            },
            {
                "task": "Find current weather data and send a report via email",
                "available_tools": ["web_search", "api_client", "email_sender"],
                "optimal_selection": ["api_client", "email_sender"],
                "reasoning": "Use weather API, then send email with results"
            },
            {
                "task": "Debug this Python code and fix the errors",
                "available_tools": ["python_executor", "code_analyzer", "file_reader"],
                "optimal_selection": ["code_analyzer", "python_executor"],
                "reasoning": "Analyze code for issues, then test fixes"
            }
        ]
        
        for scenario in selection_scenarios:
            example = ToolTrainingExample(
                task_description=scenario["task"],
                input_data={
                    "available_tools": scenario["available_tools"],
                    "scenario": scenario["task"]
                },
                expected_tools=scenario["optimal_selection"],
                tool_sequence=[
                    {"tool_id": tool, "action": "select", "reasoning": scenario["reasoning"]}
                    for tool in scenario["optimal_selection"]
                ],
                expected_output=f"Selected tools: {', '.join(scenario['optimal_selection'])}. Reasoning: {scenario['reasoning']}",
                success_criteria={"optimal_selection": True, "reasoning_provided": True},
                tool_pattern=ToolUsagePattern.SEQUENTIAL,
                difficulty_level=0.7
            )
            
            self.examples.append(example)
    
    async def _generate_coordination_examples(self):
        """Generate examples for multi-tool coordination"""
        coordination_workflows = [
            {
                "task": "Research a topic, analyze the data, and create a presentation",
                "workflow": [
                    {"tool": "web_search", "purpose": "gather information"},
                    {"tool": "python_executor", "purpose": "analyze data"},
                    {"tool": "data_visualizer", "purpose": "create charts"},
                    {"tool": "file_writer", "purpose": "save presentation"}
                ],
                "coordination_type": "sequential",
                "dependencies": {"step2": ["step1"], "step3": ["step1", "step2"], "step4": ["step3"]}
            },
            {
                "task": "Process multiple files in parallel and compile results",
                "workflow": [
                    {"tool": "file_reader", "purpose": "read file1", "parallel_group": "read"},
                    {"tool": "file_reader", "purpose": "read file2", "parallel_group": "read"},
                    {"tool": "file_reader", "purpose": "read file3", "parallel_group": "read"},
                    {"tool": "python_executor", "purpose": "compile results"}
                ],
                "coordination_type": "parallel_then_sequential",
                "dependencies": {"step4": ["step1", "step2", "step3"]}
            }
        ]
        
        for workflow in coordination_workflows:
            example = ToolTrainingExample(
                task_description=workflow["task"],
                input_data={
                    "workflow_type": workflow["coordination_type"],
                    "dependencies": workflow["dependencies"]
                },
                expected_tools=[step["tool"] for step in workflow["workflow"]],
                tool_sequence=[
                    {
                        "tool_id": step["tool"],
                        "action": "coordinate",
                        "purpose": step["purpose"],
                        "parallel_group": step.get("parallel_group"),
                        "dependencies": workflow["dependencies"]
                    }
                    for step in workflow["workflow"]
                ],
                expected_output=f"Coordinated workflow: {workflow['coordination_type']} execution of {len(workflow['workflow'])} tools",
                success_criteria={"coordination_correct": True, "dependencies_respected": True},
                tool_pattern=ToolUsagePattern.PARALLEL if "parallel" in workflow["coordination_type"] else ToolUsagePattern.SEQUENTIAL,
                difficulty_level=0.8
            )
            
            self.examples.append(example)
    
    async def _generate_optimization_examples(self):
        """Generate examples for workflow optimization"""
        optimization_scenarios = [
            {
                "task": "Optimize this data processing workflow for speed",
                "original_workflow": ["file_reader", "python_executor", "python_executor", "file_writer"],
                "optimized_workflow": ["file_reader", "python_executor", "file_writer"],
                "optimization": "Combined processing steps to reduce redundancy",
                "improvement": "50% faster execution"
            },
            {
                "task": "Minimize API calls while gathering comprehensive data",
                "original_workflow": ["api_client", "api_client", "api_client", "data_merger"],
                "optimized_workflow": ["api_client", "data_merger"],
                "optimization": "Use batch API call instead of multiple individual calls",
                "improvement": "66% fewer API calls"
            }
        ]
        
        for scenario in optimization_scenarios:
            example = ToolTrainingExample(
                task_description=scenario["task"],
                input_data={
                    "original_workflow": scenario["original_workflow"],
                    "optimization_target": "speed" if "speed" in scenario["task"] else "cost"
                },
                expected_tools=scenario["optimized_workflow"],
                tool_sequence=[
                    {
                        "tool_id": tool,
                        "action": "optimize",
                        "optimization_reason": scenario["optimization"]
                    }
                    for tool in scenario["optimized_workflow"]
                ],
                expected_output=f"Optimized workflow: {scenario['optimization']}. Result: {scenario['improvement']}",
                success_criteria={"optimization_applied": True, "improvement_achieved": True},
                tool_pattern=ToolUsagePattern.SEQUENTIAL,
                difficulty_level=0.9
            )
            
            self.examples.append(example)
    
    async def _generate_safety_examples(self):
        """Generate examples for safety and error handling"""
        safety_scenarios = [
            {
                "task": "Handle a tool execution failure gracefully",
                "primary_tool": "api_client",
                "failure_mode": "network_timeout",
                "fallback_strategy": ["retry_with_backoff", "use_alternative_api"],
                "safety_check": "validate_response_before_use"
            },
            {
                "task": "Execute potentially dangerous file operations safely",
                "primary_tool": "file_writer",
                "failure_mode": "permission_denied",
                "fallback_strategy": ["check_permissions", "use_temp_directory"],
                "safety_check": "validate_file_path"
            },
            {
                "task": "Handle sensitive data processing with privacy protection",
                "primary_tool": "data_processor",
                "failure_mode": "data_leak_risk",
                "fallback_strategy": ["anonymize_data", "use_secure_sandbox"],
                "safety_check": "scan_for_pii"
            }
        ]
        
        for scenario in safety_scenarios:
            example = ToolTrainingExample(
                task_description=scenario["task"],
                input_data={
                    "primary_tool": scenario["primary_tool"],
                    "risk_level": "high",
                    "safety_required": True
                },
                expected_tools=[scenario["primary_tool"]] + scenario["fallback_strategy"],
                tool_sequence=[
                    {
                        "tool_id": scenario["primary_tool"],
                        "action": "execute_safely",
                        "safety_checks": [scenario["safety_check"]],
                        "fallback_plan": scenario["fallback_strategy"]
                    }
                ],
                expected_output=f"Safe execution with fallback: {scenario['fallback_strategy']}",
                success_criteria={"safety_checks_performed": True, "fallback_ready": True},
                tool_pattern=ToolUsagePattern.CONDITIONAL,
                safety_requirements=[scenario["safety_check"]],
                difficulty_level=0.85
            )
            
            self.examples.append(example)
    
    async def _execute_tool_interactions(self):
        """Execute tool interactions to create ground truth data"""
        logger.info("Executing tool interactions for ground truth",
                   examples=len(self.examples))
        
        for i, example in enumerate(self.examples):
            if i % 50 == 0:
                logger.info("Processing tool interactions",
                           progress=f"{i}/{len(self.examples)}")
            
            try:
                # Execute the tool sequence to get real results
                execution_results = []
                
                for tool_step in example.tool_sequence:
                    if tool_step.get("action") == "execute":
                        # Create tool execution request
                        execution_request = ToolExecutionRequest(
                            tool_id=tool_step["tool_id"],
                            tool_action="execute",
                            parameters=tool_step.get("parameters", {}),
                            user_id="training_system"
                        )
                        
                        # Execute in sandbox for safety
                        result = await self.sandbox_manager.execute_tool_safely(execution_request)
                        execution_results.append(result)
                
                # Store execution results in example
                example.input_data["execution_results"] = execution_results
                
            except Exception as e:
                logger.warning("Tool execution failed for training example",
                             example_index=i,
                             error=str(e))
                # Continue with other examples
                continue
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # Format for training
        return {
            "task_description": example.task_description,
            "input_data": json.dumps(example.input_data),
            "expected_tools": json.dumps(example.expected_tools),
            "tool_sequence": json.dumps(example.tool_sequence),
            "expected_output": example.expected_output,
            "success_criteria": json.dumps(example.success_criteria),
            "difficulty_level": example.difficulty_level,
            "tool_pattern": example.tool_pattern.value,
            "safety_requirements": json.dumps(example.safety_requirements)
        }


class ToolAwareTrainingPipeline(ProductionTrainingPipeline):
    """
    Enhanced Training Pipeline with Tool-Aware Capabilities
    
    Extends the production training pipeline to include tool use training,
    enabling distilled models to learn effective tool utilization patterns.
    
    Key Features:
    - Tool-aware dataset generation with real tool interactions
    - Multi-strategy training for different tool usage patterns
    - Safety-focused training with error handling
    - Integration with MCP tool ecosystem
    - Comprehensive tool usage analytics
    """
    
    def __init__(self, config: TrainingConfig = None):
        super().__init__(config)
        
        # Tool-specific components
        self.tool_router = ToolRouter()
        self.sandbox_manager = SandboxManager()
        self.ftns_service = FTNSService()
        
        # Tool training configuration
        self.tool_training_enabled = getattr(settings, "PRSM_ENABLE_TOOL_TRAINING", True)
        self.tool_training_ratio = 0.3  # 30% of training focused on tool usage
        
        # Tool training metrics
        self.tool_training_metrics = {
            "tool_selection_accuracy": 0.0,
            "tool_coordination_success": 0.0,
            "safety_compliance": 0.0,
            "optimization_effectiveness": 0.0
        }
        
        logger.info("Tool-aware training pipeline initialized",
                   tool_training_enabled=self.tool_training_enabled,
                   tool_training_ratio=self.tool_training_ratio)
    
    async def run_tool_aware_training(self, job_id: str, teacher_model_name: str,
                                    student_model_path: str, domain: str,
                                    training_strategy: ToolTrainingStrategy = ToolTrainingStrategy.BASIC_AWARENESS) -> Dict[str, Any]:
        """
        Execute tool-aware training with specified strategy
        
        Args:
            job_id: Training job identifier
            teacher_model_name: Name of teacher model
            student_model_path: Path to student model
            domain: Training domain
            training_strategy: Tool training strategy
            
        Returns:
            Training results with tool usage metrics
        """
        if not self.tool_training_enabled:
            logger.warning("Tool training is disabled, falling back to standard training")
            return await self.run_distillation_training(job_id, teacher_model_name, student_model_path, domain)
        
        try:
            logger.info("Starting tool-aware training",
                       job_id=job_id,
                       teacher_model=teacher_model_name,
                       domain=domain,
                       strategy=training_strategy.value)
            
            # 1. Load student model
            student_model, tokenizer = await self._load_student_model(student_model_path, domain)
            student_model.to(self.device)
            
            # 2. Create tool-aware dataset
            tool_dataset = ToolAwareDataset(
                self.tool_router, self.sandbox_manager, domain, training_strategy,
                size=int(1000 * self.tool_training_ratio), tokenizer=tokenizer
            )
            await tool_dataset.initialize()
            
            # 3. Create standard dataset for mixed training
            standard_dataset = await self._create_standard_dataset(teacher_model_name, domain, tokenizer)
            
            # 4. Create mixed training loader
            mixed_dataloader = await self._create_mixed_dataloader(tool_dataset, standard_dataset)
            
            # 5. Initialize tool-aware optimizer
            optimizer = await self._create_tool_aware_optimizer(student_model)
            
            # 6. Tool-aware training loop
            training_results = await self._tool_aware_training_loop(
                student_model, mixed_dataloader, optimizer, job_id, training_strategy
            )
            
            # 7. Evaluate tool capabilities
            tool_evaluation = await self._evaluate_tool_capabilities(student_model, tokenizer, domain)
            
            # 8. Save enhanced model
            model_save_path = await self._save_tool_aware_model(student_model, tokenizer, job_id, tool_evaluation)
            
            logger.info("Tool-aware training completed",
                       job_id=job_id,
                       tool_selection_accuracy=self.tool_training_metrics["tool_selection_accuracy"],
                       safety_compliance=self.tool_training_metrics["safety_compliance"])
            
            return {
                "status": "completed",
                "model_path": model_save_path,
                "training_strategy": training_strategy.value,
                "tool_training_metrics": self.tool_training_metrics,
                "tool_evaluation": tool_evaluation,
                "training_results": training_results
            }
            
        except Exception as e:
            logger.error("Tool-aware training failed",
                        job_id=job_id,
                        error=str(e))
            return {
                "status": "failed",
                "error": str(e)
            }
    
    async def _create_mixed_dataloader(self, tool_dataset: ToolAwareDataset, 
                                     standard_dataset) -> DataLoader:
        """Create mixed dataloader combining tool and standard training"""
        # Combine datasets with proper weighting
        from torch.utils.data import ConcatDataset
        
        # Weight tool examples higher for better learning
        tool_weight = 2  # Each tool example counts as 2 standard examples
        weighted_tool_dataset = [tool_dataset] * tool_weight
        
        combined_dataset = ConcatDataset([*weighted_tool_dataset, standard_dataset])
        
        return DataLoader(
            combined_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.dataloader_num_workers,
            collate_fn=self._tool_aware_collate_fn
        )
    
    def _tool_aware_collate_fn(self, batch):
        """Enhanced collate function for tool-aware training"""
        # Separate tool examples from standard examples
        tool_examples = []
        standard_examples = []
        
        for item in batch:
            if "tool_sequence" in item:
                tool_examples.append(item)
            else:
                standard_examples.append(item)
        
        # Process each type separately
        tool_batch = self._process_tool_batch(tool_examples) if tool_examples else None
        standard_batch = self._collate_fn(standard_examples) if standard_examples else None
        
        return {
            "tool_batch": tool_batch,
            "standard_batch": standard_batch,
            "batch_type": "mixed"
        }
    
    def _process_tool_batch(self, tool_examples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Process tool-specific training examples"""
        if not tool_examples:
            return None
        
        # Extract tool-specific features
        tasks = [ex["task_description"] for ex in tool_examples]
        expected_tools = [json.loads(ex["expected_tools"]) for ex in tool_examples]
        tool_sequences = [json.loads(ex["tool_sequence"]) for ex in tool_examples]
        patterns = [ex["tool_pattern"] for ex in tool_examples]
        
        # Tokenize tasks
        if self.tokenizer:
            tokenized = self.tokenizer(
                tasks, 
                padding=True, 
                truncation=True, 
                return_tensors="pt",
                max_length=512
            )
        else:
            tokenized = {"input_ids": None, "attention_mask": None}
        
        return {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "expected_tools": expected_tools,
            "tool_sequences": tool_sequences,
            "tool_patterns": patterns,
            "batch_size": len(tool_examples)
        }
    
    async def _tool_aware_training_loop(self, model, dataloader, optimizer, 
                                      job_id: str, strategy: ToolTrainingStrategy) -> Dict[str, Any]:
        """Enhanced training loop with tool-aware objectives"""
        model.train()
        total_loss = 0.0
        tool_specific_losses = {
            "tool_selection": 0.0,
            "tool_coordination": 0.0,
            "safety_compliance": 0.0
        }
        
        num_batches = len(dataloader)
        start_time = time.time()
        
        for epoch in range(self.config.num_epochs):
            epoch_loss = 0.0
            
            for batch_idx, batch in enumerate(dataloader):
                optimizer.zero_grad()
                
                # Calculate mixed loss
                loss, loss_components = await self._calculate_tool_aware_loss(model, batch, strategy)
                
                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.max_grad_norm)
                optimizer.step()
                
                # Update metrics
                epoch_loss += loss.item()
                for component, value in loss_components.items():
                    if component in tool_specific_losses:
                        tool_specific_losses[component] += value
                
                # Log progress
                if batch_idx % self.config.logging_steps == 0:
                    logger.info("Training progress",
                               job_id=job_id,
                               epoch=epoch,
                               batch=f"{batch_idx}/{num_batches}",
                               loss=loss.item(),
                               tool_selection_loss=loss_components.get("tool_selection", 0.0))
            
            # Epoch completed
            avg_epoch_loss = epoch_loss / num_batches
            logger.info("Epoch completed",
                       job_id=job_id,
                       epoch=epoch,
                       avg_loss=avg_epoch_loss)
        
        training_time = time.time() - start_time
        
        # Calculate final metrics
        self.tool_training_metrics.update({
            "tool_selection_accuracy": 1.0 - (tool_specific_losses["tool_selection"] / num_batches),
            "tool_coordination_success": 1.0 - (tool_specific_losses["tool_coordination"] / num_batches),
            "safety_compliance": 1.0 - (tool_specific_losses["safety_compliance"] / num_batches)
        })
        
        return {
            "final_loss": total_loss / (num_batches * self.config.num_epochs),
            "tool_specific_losses": tool_specific_losses,
            "training_time": training_time,
            "epochs_completed": self.config.num_epochs
        }
    
    async def _calculate_tool_aware_loss(self, model, batch, strategy: ToolTrainingStrategy) -> Tuple[torch.Tensor, Dict[str, float]]:
        """Calculate tool-aware training loss"""
        total_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        loss_components = {}
        
        # Standard language modeling loss
        if batch.get("standard_batch"):
            std_loss = self._calculate_standard_loss(model, batch["standard_batch"])
            total_loss = total_loss + std_loss * 0.7  # 70% weight for standard training
            loss_components["standard"] = std_loss.item()
        
        # Tool-specific losses
        if batch.get("tool_batch"):
            tool_losses = await self._calculate_tool_losses(model, batch["tool_batch"], strategy)
            total_loss = total_loss + sum(tool_losses.values()) * 0.3  # 30% weight for tool training
            loss_components.update({k: v.item() for k, v in tool_losses.items()})
        
        return total_loss, loss_components
    
    async def _calculate_tool_losses(self, model, tool_batch, strategy: ToolTrainingStrategy) -> Dict[str, torch.Tensor]:
        """Calculate tool-specific training losses"""
        losses = {}
        
        if tool_batch["input_ids"] is not None:
            # Tool selection loss
            tool_selection_loss = self._calculate_tool_selection_loss(model, tool_batch)
            losses["tool_selection"] = tool_selection_loss
            
            # Strategy-specific losses
            if strategy == ToolTrainingStrategy.TOOL_COORDINATION:
                coord_loss = self._calculate_coordination_loss(model, tool_batch)
                losses["tool_coordination"] = coord_loss
            
            if strategy == ToolTrainingStrategy.SAFETY_PROTOCOLS:
                safety_loss = self._calculate_safety_loss(model, tool_batch)
                losses["safety_compliance"] = safety_loss
        
        return losses
    
    def _calculate_tool_selection_loss(self, model, tool_batch) -> torch.Tensor:
        """Calculate loss for tool selection accuracy"""
        # Simplified tool selection loss
        # In production, this would be more sophisticated
        batch_size = tool_batch["batch_size"]
        
        # Create a mock loss that decreases as model learns tool selection
        # This would be replaced with actual tool selection prediction loss
        mock_loss = torch.tensor(0.1, device=self.device, requires_grad=True)
        
        return mock_loss
    
    def _calculate_coordination_loss(self, model, tool_batch) -> torch.Tensor:
        """Calculate loss for tool coordination learning"""
        # Mock coordination loss
        return torch.tensor(0.05, device=self.device, requires_grad=True)
    
    def _calculate_safety_loss(self, model, tool_batch) -> torch.Tensor:
        """Calculate loss for safety protocol compliance"""
        # Mock safety loss
        return torch.tensor(0.02, device=self.device, requires_grad=True)
    
    async def _evaluate_tool_capabilities(self, model, tokenizer, domain: str) -> Dict[str, Any]:
        """Evaluate model's tool usage capabilities"""
        model.eval()
        
        evaluation_results = {
            "tool_awareness_score": 0.0,
            "tool_selection_accuracy": 0.0,
            "safety_compliance_rate": 0.0,
            "coordination_effectiveness": 0.0
        }
        
        # Create evaluation dataset
        eval_dataset = ToolAwareDataset(
            self.tool_router, self.sandbox_manager, domain,
            ToolTrainingStrategy.BASIC_AWARENESS, size=100, tokenizer=tokenizer
        )
        await eval_dataset.initialize()
        
        correct_selections = 0
        total_evaluations = len(eval_dataset)
        
        for i in range(min(50, total_evaluations)):  # Evaluate on subset
            example = eval_dataset[i]
            
            # Generate model response
            task_prompt = f"Task: {example['task_description']}\nRequired tools:"
            
            try:
                with torch.no_grad():
                    inputs = tokenizer(task_prompt, return_tensors="pt").to(self.device)
                    outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.1)
                    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Simple evaluation: check if expected tools are mentioned
                expected_tools = json.loads(example["expected_tools"])
                tools_mentioned = sum(1 for tool in expected_tools if tool in response.lower())
                
                if tools_mentioned > 0:
                    correct_selections += 1
                
            except Exception as e:
                logger.warning("Evaluation failed for example", index=i, error=str(e))
                continue
        
        # Calculate metrics
        evaluation_results["tool_selection_accuracy"] = correct_selections / min(50, total_evaluations)
        evaluation_results["tool_awareness_score"] = evaluation_results["tool_selection_accuracy"] * 0.8
        evaluation_results["safety_compliance_rate"] = 0.95  # Mock high safety compliance
        evaluation_results["coordination_effectiveness"] = 0.75  # Mock coordination score
        
        logger.info("Tool capability evaluation completed",
                   domain=domain,
                   tool_selection_accuracy=evaluation_results["tool_selection_accuracy"],
                   tool_awareness_score=evaluation_results["tool_awareness_score"])
        
        return evaluation_results
    
    async def _save_tool_aware_model(self, model, tokenizer, job_id: str, 
                                   evaluation: Dict[str, Any]) -> str:
        """Save tool-aware model with metadata"""
        save_path = f"./models/tool_aware_{job_id}"
        Path(save_path).mkdir(parents=True, exist_ok=True)
        
        # Save model and tokenizer
        model.save_pretrained(save_path)
        tokenizer.save_pretrained(save_path)
        
        # Save tool-aware metadata
        metadata = {
            "model_type": "tool_aware_distilled",
            "training_timestamp": datetime.now(timezone.utc).isoformat(),
            "tool_capabilities": evaluation,
            "tool_training_metrics": self.tool_training_metrics,
            "available_tools": [
                tool_id for tool_id in self.tool_router.tool_registry.tools.keys()
            ]
        }
        
        with open(f"{save_path}/tool_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info("Tool-aware model saved",
                   job_id=job_id,
                   save_path=save_path,
                   tool_capabilities=evaluation)
        
        return save_path
    
    async def get_tool_training_analytics(self) -> Dict[str, Any]:
        """Get comprehensive tool training analytics"""
        return {
            "tool_training_enabled": self.tool_training_enabled,
            "tool_training_ratio": self.tool_training_ratio,
            "current_metrics": self.tool_training_metrics,
            "available_strategies": [strategy.value for strategy in ToolTrainingStrategy],
            "supported_patterns": [pattern.value for pattern in ToolUsagePattern],
            "tool_router_analytics": self.tool_router.get_tool_analytics(),
            "marketplace_integration": tool_marketplace.get_marketplace_stats()
        }


# Factory function
def create_tool_aware_pipeline(config: TrainingConfig = None) -> ToolAwareTrainingPipeline:
    """Create tool-aware training pipeline"""
    return ToolAwareTrainingPipeline(config)


# Global pipeline instance
tool_aware_pipeline = None

def get_tool_aware_pipeline() -> ToolAwareTrainingPipeline:
    """Get or create global tool-aware training pipeline"""
    global tool_aware_pipeline
    if tool_aware_pipeline is None:
        tool_aware_pipeline = ToolAwareTrainingPipeline()
    return tool_aware_pipeline