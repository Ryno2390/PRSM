#!/usr/bin/env python3
"""
Bulk Dataset Ingestion for PRSM
===============================

This approach downloads pre-compiled academic datasets instead of
individual papers, dramatically improving speed.
"""

import asyncio
import aiohttp
import json
import gzip
from pathlib import Path
from typing import Dict, List, Any
import tarfile
import zipfile

class BulkDatasetIngestion:
    """Download bulk academic datasets for fast ingestion"""
    
    def __init__(self):
        self.storage_path = Path("/Volumes/My Passport/PRSM_Storage")
        self.bulk_datasets = self._get_bulk_datasets()
    
    def _get_bulk_datasets(self) -> List[Dict[str, Any]]:
        """Get list of bulk academic datasets"""
        
        return [
            {
                "name": "OpenAlex Snapshot",
                "description": "Complete academic paper database",
                "url": "https://openalex.org/download",
                "estimated_papers": 200000000,  # 200M papers
                "format": "jsonl.gz",
                "size_gb": 50,
                "domains": ["all"]
            },
            {
                "name": "Semantic Scholar Open Research Corpus",
                "description": "Academic papers with abstracts",
                "url": "https://api.semanticscholar.org/corpus/download",
                "estimated_papers": 200000000,
                "format": "jsonl.gz", 
                "size_gb": 30,
                "domains": ["computer_science", "medicine", "biology"]
            },
            {
                "name": "arXiv Dataset",
                "description": "Complete arXiv papers dataset",
                "url": "https://www.kaggle.com/datasets/Cornell-University/arxiv",
                "estimated_papers": 2000000,
                "format": "json",
                "size_gb": 5,
                "domains": ["physics", "cs", "math", "biology"]
            },
            {
                "name": "PubMed Central Open Access",
                "description": "Full-text biomedical papers",
                "url": "https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/",
                "estimated_papers": 3000000,
                "format": "xml",
                "size_gb": 20,
                "domains": ["medicine", "biology"]
            },
            {
                "name": "CORE Academic Papers",
                "description": "Open access research papers",
                "url": "https://core.ac.uk/services/dataset",
                "estimated_papers": 30000000,
                "format": "jsonl",
                "size_gb": 15,
                "domains": ["multidisciplinary"]
            }
        ]
    
    def display_dataset_options(self):
        """Display available bulk datasets"""
        
        print("ğŸ“Š BULK ACADEMIC DATASETS AVAILABLE")
        print("=" * 60)
        print("ğŸš€ MUCH FASTER than individual paper downloads!")
        print()
        
        for i, dataset in enumerate(self.bulk_datasets, 1):
            print(f"{i}. {dataset['name']}")
            print(f"   ğŸ“„ Papers: {dataset['estimated_papers']:,}")
            print(f"   ğŸ’¾ Size: {dataset['size_gb']} GB")
            print(f"   ğŸŒ Domains: {', '.join(dataset['domains'])}")
            print(f"   ğŸ”— Source: {dataset['url']}")
            print()
        
        print("ğŸ’¡ RECOMMENDATION:")
        print("   ğŸ¯ For 150k papers: Download arXiv dataset (#3)")
        print("   âš¡ Speed: Minutes instead of days!")
        print("   ğŸ“Š Quality: Pre-validated academic content")
        print("   ğŸŒ Breadth: Multiple domains covered")
        print()
        
        total_papers = sum(d['estimated_papers'] for d in self.bulk_datasets)
        total_size = sum(d['size_gb'] for d in self.bulk_datasets)
        
        print(f"ğŸ“Š TOTAL AVAILABLE: {total_papers:,} papers, {total_size} GB")
        print("=" * 60)
    
    async def download_arxiv_dataset(self):
        """Download and process arXiv dataset (recommended for speed)"""
        
        print("ğŸš€ DOWNLOADING ARXIV DATASET")
        print("=" * 40)
        print("ğŸ“„ ~2M papers covering CS, Physics, Math, Biology")
        print("ğŸ’¾ ~5GB download size")
        print("âš¡ Estimated download time: 10-30 minutes")
        print("ğŸ¯ Will easily exceed 150k paper target")
        print()
        
        # This would implement the actual download
        # For now, showing the approach
        
        print("ğŸ“ MANUAL DOWNLOAD INSTRUCTIONS:")
        print("1. Go to: https://www.kaggle.com/datasets/Cornell-University/arxiv")
        print("2. Download the dataset (requires free Kaggle account)")
        print("3. Extract to: /Volumes/My Passport/PRSM_Storage/arxiv_dataset/")
        print("4. Run: python process_arxiv_dataset.py")
        print()
        print("âš¡ This will be 1000x faster than individual downloads!")
        
        return True


def main():
    """Main function"""
    
    print("ğŸš€ BULK DATASET INGESTION SOLUTION")
    print("=" * 50)
    print("âŒ Individual paper downloading is too slow")
    print("âœ… Bulk dataset downloading is the solution")
    print()
    
    ingestion = BulkDatasetIngestion()
    ingestion.display_dataset_options()
    
    print("ğŸ¯ IMMEDIATE ACTION RECOMMENDED:")
    print("   Stop current slow ingestion")
    print("   Download arXiv dataset instead")
    print("   Get 150k+ papers in under 1 hour")
    print()


if __name__ == "__main__":
    main()