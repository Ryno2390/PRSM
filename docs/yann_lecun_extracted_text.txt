
--- PAGE 1 ---
NLP|Computer VisionYann LeCun on a vision to make AIsystems learn and reason like animalsand humansFebruary 23, 2022
Share on FacebookShare on Twitter
6/27/25, 1:28 PMPage 1 of 12

--- PAGE 2 ---
Watch the Meta AI Inside the Lab event .For all the remarkable recent progress in AI research, we are still very farfrom creating machines that think and learn as well as people do. As MetaAI’s Chief AI Scientist Yann LeCun notes, a teenager who has never satbehind a steering wheel can learn to drive in about 20 hours, while thebest autonomous driving systems today need millions or billions of piecesof labeled training data and millions of reinforcement learning trials invirtual environments. And even then, they fall short of human’s ability todrive a car reliably.What will it take to build AI that approaches human-level capabilities? Is itsimply a matter of more data and bigger AI models?As part of Meta AI’s Inside the Lab event on February 23, 2022, LeCun issketching an alternate vision for building human-level AI. LeCun proposesthat the ability to learn “world models” — internal models of how theworld works — may be the key.Meta AI is sharing some of LeCun’s ideas in brief here, including hisproposal for a modular, configurable architecture for autonomousintelligence, as well as key challenges the AI research community mustaddress to build such a system. We typically share the results of ourhere6/27/25, 1:28 PMPage 2 of 12

--- PAGE 3 ---
research — by publishing papers, code, and data sets, as well as blog posts— when they are completed. But in keeping with Meta AI’s open-scienceapproach, we are taking this opportunity to preview our research visionand ideas in the hope that it spurs discussion and collaboration among AIresearchers. The simple fact is that we will need to work together to solvethese extraordinarily challenging, exciting problems.We plan to share more details on LeCun’s vision in an upcoming positionpaper.AI that can model how the world works“Human and nonhuman animals seem able to learn enormous amounts ofbackground knowledge about how the world works through observationand through an incomprehensibly small amount of interactions in a task-independent, unsupervised way,” LeCun says. “It can be hypothesized thatthis accumulated knowledge may constitute the basis for what is oftencalled common sense.”And common sense can be seen as a collection of models of the worldthat can guide on what is likely, what is plausible, and what is impossible.This allows humans to plan effectively in unfamiliar situations. That teendriver may not have driven over snow before, for example, but he(hopefully) knows that snow can be slippery and send his car into a skid hedrives too aggressively.Common sense knowledge allows animals not just to predict futureoutcomes but also to fill in missing information, whether temporally orspatially. When a driver hears the sound of metal smashing togethernearby, he knows immediately that there’s been an accident — evenwithout seeing the vehicles involved.The idea that humans, animals, and intelligent systems use world models6/27/25, 1:28 PMPage 3 of 12

--- PAGE 4 ---
The idea that humans, animals, and intelligent systems use world modelsgoes back many decades in psychology and in fields of engineering suchas control and robotics. LeCun proposes that one of the most importantchallenges in AI today is devising learning paradigms and architecturesthat would allow machines to learn world models in a self-supervisedfashion and then use those models to predict, reason, and plan. His outlineregroups ideas that have been proposed in various disciplines, such ascognitive science, systems neuroscience, optimal control, reinforcementlearning, and “traditional” AI, and combines them with new concepts inmachine learning, such as self-supervised learning and joint-embeddingarchitectures.Proposing an architecture for autonomousintelligenceLeCun proposes an architecture composed of six separate modules. Eachis assumed to be differentiable, in that it can easily compute gradientestimates of some objective function with respect to its own input andpropagate the gradient information to upstream modules.
6/27/25, 1:28 PMPage 4 of 12

--- PAGE 5 ---
A system architecture for autonomous intelligence. The configurator gets inputs fromother modules, but we have omitted those arrows in order to simplify the diagram.The configurator module performs executive control: Given a task tobe executed, it preconfigures the perception module, the worldmodel, the cost, and the actor for the task at hand, possibly bymodulating the parameters of those modules.The perception module receives signals from sensors and estimatesthe current state of the world. For a given task, only a small subset ofthe perceived state of the world is relevant and useful. Theconfigurator module primes the perception system to extract therelevant information from the percept for the task at hand.The world model module constitutes the most complex piece of thearchitecture. Its role is twofold: (1) to estimate missing informationabout the state of the world not provided by perception, and (2) topredict plausible future states of the world. The world model maypredict natural evolutions of the world or predict future world statesresulting from a sequence of actions proposed by the actor module.The world model is a kind of simulator of the part of the worldrelevant to the task at hand. Since the world is full of uncertainty, themodel must be able to represent multiple possible predictions. Adriver approaching an intersection may slow down in case anothercar approaching the intersection doesn’t stop at the stop sign.The cost module computes a single scalar output that predicts thelevel of discomfort of the agent. It is composed of two submodules:the intrinsic cost, which is hard-wired and immutable (not trainable),and computes the immediate discomfort (such as damage to theagent, violation of hard-coded behavioral constraints, etc.), and thecritic, which is a trainable module that predicts future values of theintrinsic cost. The ultimate goal of the agent is to minimize theintrinsic cost over the long run. “This is where basic behavioral drivesand intrinsic motivations reside,” LeCun says. So it will factor in6/27/25, 1:28 PMPage 5 of 12

--- PAGE 6 ---
intrinsic costs, such as not wasting energy, as well as costs specific tothe task at hand. “Because the cost module is differentiable, thegradient of the cost can be back-propagated through the othermodules for planning, reasoning, or learning.”The actor module computes proposals for action sequences. “Theactor can find an optimal action sequence that minimizes theestimated future cost, and output the first action in the optimalsequence, in a fashion similar to classical optimal control,” LeCunsays.The short-term memory module keeps track of the current andpredicted world state, as well as associated costs.World model architecture and self-supervisedtrainingThe centerpiece of the architecture is the predictive world model. Acritical challenge with constructing it is how to enable it to representmultiple plausible predictions. The real world is not entirely predictable:There are many possible ways a particular situation can evolve, and thereare many details of a situation that are irrelevant to the task at hand. I mayneed to anticipate what cars around me are going to do while I drive, but Idon’t need to predict the detailed position of individual leaves in the treesthat are near the road. How can a world model learn abstractrepresentations of the world so that important details are preserved,irrelevant details are ignored, and predictions can be performed in thespace of abstract representations?One key element of a solution is the Joint Embedding PredictiveArchitecture (JEPA). The JEPA captures the dependencies between twoinputs, x and y. For example, x could be a segment of video, and y the nextsegment of the video. Inputs x and y are fed to trainable encoders thatextract abstract representations of them, sx and sy. A predictor module istrained to predict sy from sx. The predictor may use a latent variable, z, to6/27/25, 1:28 PMPage 6 of 12

--- PAGE 7 ---
represent information present in sy that is not present in sx. The JEPAhandles uncertainty in predictions in two ways: (1) The encoder maychoose to drop information about y that is difficult to predict, (2) thelatent variable z, when varied over a set, will cause the prediction to varyover a set of plausible predictions.How do we train a JEPA? Until recently, the only approach would havebeen to use contrastive methods, which consist of showing examples ofcompatible x and y, together with numerous examples of x andincompatible y’s. But this is rather impractical when the representationsare high-dimensional. An alternative training strategy has emerged in thelast two years: regularized methods. When applied to JEPA, the methoduses four criteria:1. Make the representation of x maximally informative about x2. Make the representation of y maximally informative about y3. Make the representation of y maximally predictable from therepresentation of x4. Make the predictor use as little information as possible from thelatent variable to represent uncertainty in the prediction.These criteria can be translated into differentiable cost functions invarious ways. One way is the , which stands for Variance,Invariance, Covariance Regularization. In VICReg, the information contentof the representations of x and y are maximized by maintaining thevariances of their components over a threshold and by making thesecomponents as independent of each other as possible. Simultaneously,the model tries to make the representation of y predictable from that of x.Additionally, the information content of the latent variable is minimized bymaking it discrete, low-dimensional, sparse, or noisy.
VICReg method6/27/25, 1:28 PMPage 7 of 12

--- PAGE 8 ---
The beauty of the JEPA is that it naturally produces informative abstractrepresentations of the input that eliminate irrelevant details and withwhich predictions can be performed. This enables JEPAs to be stacked ontop of one another so as to learn representations with higher levels ofabstraction that can perform longer-term prediction. For example, ascenario can be described at a high level as “a cook is making crêpes.” Onecan predict that the cook will fetch flour, milk, and eggs; mix theingredients; ladle batter into a pan; let the batter fry; flip the crêpe; andrepeat. At a lower level, pouring a ladle involves scooping some batter andspreading it around the pan. This continues all the way down to theprecise trajectories of the chef’s hands millisecond by millisecond. At thelow level of hand trajectories, our world model can only make accuratepredictions in the short term. But at a higher level of abstraction, it canmake long-term predictions.
6/27/25, 1:28 PMPage 8 of 12

--- PAGE 9 ---
The Hierarchical JEPA can be used to perform predictions at several levelsof abstraction and several time scales. How can it be trained? Largely bypassive observation, and less often by interaction.A baby learns how the world works largely by observation in the first fewmonths of life. She learns that the world is three-dimensional, that someobjects are in front of others, that when an object is occluded it still exists.Eventually, around nine months of age, babies learn intuitive physics — forexample, that unsupported objects fall through gravity.The hope is that a hierarchical JEPA could learn how the world works bywatching videos and interacting with its environment. By training itself topredict what will happen in the video, it will produce hierarchicalrepresentations of the world. By taking actions in the world and observingthe result, the world model will learn to predict the consequences of itsactions, which will allow it to reason and plan.A perception-action episodeWith a Hierarchical JEPA properly trained as a world model, an agentcould perform hierarchical planning of complex actions, decomposing a6/27/25, 1:28 PMPage 9 of 12

--- PAGE 10 ---
complex task into a sequence of less complex and less abstract subtasks,all the way down to low-level actions on effectors.
A typical perception-action episode would go as follows. The diagramillustrates the situation for a two-level hierarchy. The perception moduleextracts a hierarchical representation of the state of the world(s1[0]=Enc1(x) and s2[0]=Enc2(s[0]) in the diagram). Then, the second-level predictor is applied multiple times to predict future states, given asequence of abstract actions proposed by the second-level actor. Theactor optimizes the second-level action sequence so as to minimize theoverall cost (C(s2[4]) in the diagram). This process is akin to Model-Predictive Control in optimal control. The process is repeated for multipledrawings of the second-level latent variables, which may producedifferent high-level scenarios. The resulting high-level actions do notconstitute real actions but merely define constraints that the lower-levelstate sequence must satisfy (e.g., are the ingredients properly mixed?).They really constitute subgoals. The entire process is repeated at thelower level: running the lower-level predictor, optimizing the low-level6/27/25, 1:28 PMPage 10 of 12

--- PAGE 11 ---
action sequence to minimize the intermediate costs coming from theupper layer, and repeating the process for multiple drawings of the low-level latent variables. Once the process is complete, the agent outputs thefirst low-level action to the effectors, and the whole episode can berepeated.If we are successful at building such a model, all the modules would bedifferentiable, so that this whole action optimization process could beperformed using gradient-based methods.Moving closer to human-level intelligence in AILeCun’s vision requires much deeper exploration than is possible in a briefblog post, and many difficult challenges lie ahead. One of the mostinteresting and difficult of these is instantiating the details of thearchitectures and training procedures for the world model. In fact, it couldbe argued that training world models constitutes the main challengetoward real progress in AI over the next decades.But many other aspects of the architecture are still to be defined,including how precisely to train the critic, how to construct and train theconfigurator, and how to use the short-term memory to keep track of theworld state and to store a history of world states, actions, and associatedintrinsic cost to tune the critic.LeCun and other Meta AI researchers look forward to exploring these inthe months and years ahead, as well as exchanging ideas and learningfrom others in the field. Creating machines that can learn and understandas effectively as humans is a long-term scientific endeavor — and one withno guarantees of success. But we are confident that fundamentalresearch will continue to produce a deeper understanding of both mindsand machines, and will lead to advances that benefit everyone who usesAI.6/27/25, 1:28 PMPage 11 of 12

--- PAGE 12 ---
Foundational models
Meta © 2025Watch the Meta AI Inside the Lab event .here
Our approachResearchMeta AILatest news
Privacy PolicyTermsCookiesSearch AI content6/27/25, 1:28 PMPage 12 of 12
