"""
PRSM Performance Benchmark Orchestrator
Comprehensive benchmarking harness for testing PRSM performance under various load conditions

This module provides controlled, reproducible benchmarking capabilities for:
- Consensus performance under load
- P2P network scaling characteristics  
- Post-quantum signature performance impact
- Multi-region network latency analysis
- Byzantine fault tolerance testing
"""

import asyncio
import json
import time
import statistics
import random
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import csv

# Placeholder functions for testing - in production these would be proper imports
def reset_global_collector():
    """Reset the global performance collector"""
    pass

def get_global_collector():
    """Get the global performance collector"""
    class MockCollector:
        def get_all_metrics(self):
            return {}
    return MockCollector()

from uuid import uuid4


class BenchmarkType(str, Enum):
    """Types of benchmarks to run"""
    CONSENSUS_SCALING = "consensus_scaling"
    P2P_NETWORK_LOAD = "p2p_network_load"
    POST_QUANTUM_IMPACT = "post_quantum_impact"
    MULTI_REGION_LATENCY = "multi_region_latency"
    BYZANTINE_FAULT_TOLERANCE = "byzantine_fault_tolerance"
    THROUGHPUT_STRESS = "throughput_stress"


class LoadProfile(str, Enum):
    """Load testing profiles"""
    LIGHT = "light"          # 10-50 nodes, low message rate
    MODERATE = "moderate"    # 50-200 nodes, medium message rate
    HEAVY = "heavy"         # 200-500 nodes, high message rate
    EXTREME = "extreme"     # 500-1000 nodes, maximum message rate


@dataclass
class BenchmarkScenario:
    """Configuration for a benchmark scenario"""
    name: str
    benchmark_type: BenchmarkType
    load_profile: LoadProfile
    node_count: int
    duration_seconds: int
    target_tps: float  # Transactions per second
    network_latency_ms: int = 50
    packet_loss_rate: float = 0.0
    byzantine_node_ratio: float = 0.0
    enable_post_quantum: bool = True
    regions: List[str] = field(default_factory=list)  # Use str instead of RegionCode
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class BenchmarkResult:
    """Results from a benchmark run"""
    scenario_name: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    
    # Core metrics
    total_operations: int
    successful_operations: int
    failed_operations: int
    operations_per_second: float
    
    # Timing metrics
    mean_operation_time_ms: float
    median_operation_time_ms: float
    p95_operation_time_ms: float
    p99_operation_time_ms: float
    min_operation_time_ms: float
    max_operation_time_ms: float
    std_dev_ms: float
    
    # Network metrics
    total_messages: int
    message_success_rate: float
    average_network_latency_ms: float
    bandwidth_usage_mbps: float
    
    # Consensus metrics (if applicable)
    consensus_rounds: int
    consensus_success_rate: float
    average_consensus_time_ms: float
    byzantine_failures_detected: int
    
    # Resource metrics
    peak_memory_mb: float
    average_cpu_percent: float
    
    # Detailed breakdown
    operation_breakdown: Dict[str, Any] = field(default_factory=dict)
    error_breakdown: Dict[str, int] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


class BenchmarkOrchestrator:
    """
    Comprehensive benchmark orchestrator for PRSM performance testing
    
    Provides controlled, reproducible benchmarking across various scenarios
    """
    
    def __init__(self, output_directory: str = "benchmark_results"):
        """
        Initialize benchmark orchestrator
        
        Args:
            output_directory: Directory to store benchmark results
        """
        self.output_directory = Path(output_directory)
        self.output_directory.mkdir(exist_ok=True)
        
        # Benchmark history
        self.benchmark_results: List[BenchmarkResult] = []
        self.current_scenario: Optional[BenchmarkScenario] = None
        
        # Test infrastructure
        self.p2p_network: Optional[P2PModelNetwork] = None
        self.multi_region_network: Optional[MultiRegionP2PNetwork] = None
        
        # Performance tracking
        self.start_time: Optional[datetime] = None
        self.operation_count = 0
        self.error_count = 0
    
    def create_predefined_scenarios(self) -> List[BenchmarkScenario]:
        """Create a set of predefined benchmark scenarios"""
        scenarios = []
        
        # 1. Consensus Scaling Tests
        for node_count in [10, 25, 50, 100, 200]:
            scenarios.append(BenchmarkScenario(
                name=f"consensus_scaling_{node_count}_nodes",
                benchmark_type=BenchmarkType.CONSENSUS_SCALING,
                load_profile=LoadProfile.MODERATE,
                node_count=node_count,
                duration_seconds=60,
                target_tps=10.0,
                network_latency_ms=50
            ))
        
        # 2. P2P Network Load Tests
        for load_profile in [LoadProfile.LIGHT, LoadProfile.MODERATE, LoadProfile.HEAVY]:
            node_counts = {
                LoadProfile.LIGHT: 25,
                LoadProfile.MODERATE: 100, 
                LoadProfile.HEAVY: 300
            }
            tps_targets = {
                LoadProfile.LIGHT: 5.0,
                LoadProfile.MODERATE: 15.0,
                LoadProfile.HEAVY: 50.0
            }
            
            scenarios.append(BenchmarkScenario(
                name=f"p2p_load_{load_profile.value}",
                benchmark_type=BenchmarkType.P2P_NETWORK_LOAD,
                load_profile=load_profile,
                node_count=node_counts[load_profile],
                duration_seconds=120,
                target_tps=tps_targets[load_profile],
                network_latency_ms=100
            ))
        
        # 3. Post-Quantum Impact Tests
        scenarios.append(BenchmarkScenario(
            name="post_quantum_impact_comparison",
            benchmark_type=BenchmarkType.POST_QUANTUM_IMPACT,
            load_profile=LoadProfile.MODERATE,
            node_count=50,
            duration_seconds=90,
            target_tps=20.0,
            enable_post_quantum=True
        ))
        
        # 4. Multi-Region Latency Tests
        scenarios.append(BenchmarkScenario(
            name="multi_region_latency_global",
            benchmark_type=BenchmarkType.MULTI_REGION_LATENCY,
            load_profile=LoadProfile.MODERATE,
            node_count=50,  # 10 per region
            duration_seconds=180,
            target_tps=8.0,
            network_latency_ms=200,
            regions=["us-east", "us-west", "eu-central", "ap-southeast", "latam"]
        ))
        
        # 5. Byzantine Fault Tolerance Tests
        for byzantine_ratio in [0.1, 0.2, 0.33]:  # 10%, 20%, 33% Byzantine nodes
            scenarios.append(BenchmarkScenario(
                name=f"byzantine_fault_tolerance_{int(byzantine_ratio*100)}pct",
                benchmark_type=BenchmarkType.BYZANTINE_FAULT_TOLERANCE,
                load_profile=LoadProfile.MODERATE,
                node_count=100,
                duration_seconds=120,
                target_tps=10.0,
                byzantine_node_ratio=byzantine_ratio,
                network_latency_ms=75
            ))
        
        # 6. Throughput Stress Tests
        scenarios.append(BenchmarkScenario(
            name="throughput_stress_extreme",
            benchmark_type=BenchmarkType.THROUGHPUT_STRESS,
            load_profile=LoadProfile.EXTREME,
            node_count=500,
            duration_seconds=300,  # 5 minutes
            target_tps=100.0,
            network_latency_ms=150
        ))
        
        return scenarios
    
    async def setup_test_environment(self, scenario: BenchmarkScenario):
        """Setup test environment for a benchmark scenario"""
        print(f"ðŸ”§ Setting up test environment for: {scenario.name}")
        
        # Reset performance collector
        reset_global_collector()
        
        # Simulate setup - in real implementation this would create actual P2P network
        print(f"   âœ… Simulated setup for {scenario.benchmark_type.value} with {scenario.node_count} nodes")
    
    async def run_benchmark_scenario(self, scenario: BenchmarkScenario) -> BenchmarkResult:
        """Run a complete benchmark scenario"""
        print(f"\nðŸš€ Running benchmark: {scenario.name}")
        print(f"   Type: {scenario.benchmark_type.value}")
        print(f"   Load: {scenario.load_profile.value}")
        print(f"   Nodes: {scenario.node_count}")
        print(f"   Duration: {scenario.duration_seconds}s")
        print(f"   Target TPS: {scenario.target_tps}")
        
        self.current_scenario = scenario
        start_time = datetime.now(timezone.utc)
        self.start_time = start_time
        self.operation_count = 0
        self.error_count = 0
        
        # Setup test environment
        await self.setup_test_environment(scenario)
        
        # Run the specific benchmark
        if scenario.benchmark_type == BenchmarkType.CONSENSUS_SCALING:
            await self._run_consensus_scaling_benchmark(scenario)
        elif scenario.benchmark_type == BenchmarkType.P2P_NETWORK_LOAD:
            await self._run_p2p_load_benchmark(scenario)
        elif scenario.benchmark_type == BenchmarkType.POST_QUANTUM_IMPACT:
            await self._run_post_quantum_impact_benchmark(scenario)
        elif scenario.benchmark_type == BenchmarkType.MULTI_REGION_LATENCY:
            await self._run_multi_region_latency_benchmark(scenario)
        elif scenario.benchmark_type == BenchmarkType.BYZANTINE_FAULT_TOLERANCE:
            await self._run_byzantine_fault_tolerance_benchmark(scenario)
        elif scenario.benchmark_type == BenchmarkType.THROUGHPUT_STRESS:
            await self._run_throughput_stress_benchmark(scenario)
        
        # Calculate results
        end_time = datetime.now(timezone.utc)
        duration = (end_time - start_time).total_seconds()
        
        # Get performance metrics
        collector = get_global_collector()
        all_metrics = collector.get_all_metrics()
        
        # Calculate aggregate statistics
        all_times = []
        for metrics in all_metrics.values():
            # Convert milliseconds to get individual operation times
            operation_times = [metrics.mean_ms] * metrics.sample_count
            all_times.extend(operation_times)
        
        if all_times:
            mean_time = statistics.mean(all_times)
            median_time = statistics.median(all_times)
            std_dev = statistics.stdev(all_times) if len(all_times) > 1 else 0
            min_time = min(all_times)
            max_time = max(all_times)
            
            # Calculate percentiles
            sorted_times = sorted(all_times)
            p95_time = sorted_times[int(0.95 * len(sorted_times))] if sorted_times else 0
            p99_time = sorted_times[int(0.99 * len(sorted_times))] if sorted_times else 0
        else:
            mean_time = median_time = std_dev = min_time = max_time = p95_time = p99_time = 0
        
        # Create result
        result = BenchmarkResult(
            scenario_name=scenario.name,
            start_time=start_time,
            end_time=end_time,
            duration_seconds=duration,
            total_operations=self.operation_count,
            successful_operations=self.operation_count - self.error_count,
            failed_operations=self.error_count,
            operations_per_second=self.operation_count / duration if duration > 0 else 0,
            mean_operation_time_ms=mean_time,
            median_operation_time_ms=median_time,
            p95_operation_time_ms=p95_time,
            p99_operation_time_ms=p99_time,
            min_operation_time_ms=min_time,
            max_operation_time_ms=max_time,
            std_dev_ms=std_dev,
            total_messages=self.operation_count * 3,  # Estimate messages per operation
            message_success_rate=(self.operation_count - self.error_count) / max(self.operation_count, 1),
            average_network_latency_ms=scenario.network_latency_ms,
            bandwidth_usage_mbps=self.operation_count * 0.001,  # Rough estimate
            consensus_rounds=self.operation_count // 10,  # Estimate consensus rounds
            consensus_success_rate=(self.operation_count - self.error_count) / max(self.operation_count, 1),
            average_consensus_time_ms=mean_time,
            byzantine_failures_detected=0,  # Would be calculated from actual Byzantine behavior
            peak_memory_mb=100.0,  # Would be measured from actual system
            average_cpu_percent=25.0,  # Would be measured from actual system
            operation_breakdown={op: metrics.sample_count for op, metrics in all_metrics.items()},
            metadata=scenario.metadata
        )
        
        self.benchmark_results.append(result)
        
        print(f"âœ… Benchmark completed in {duration:.2f}s")
        print(f"   Operations: {result.total_operations} ({result.operations_per_second:.2f} ops/s)")
        print(f"   Success rate: {result.message_success_rate:.1%}")
        print(f"   Mean time: {result.mean_operation_time_ms:.2f}ms")
        print(f"   P95 time: {result.p95_operation_time_ms:.2f}ms")
        
        return result
    
    async def _run_consensus_scaling_benchmark(self, scenario: BenchmarkScenario):
        """Run consensus scaling benchmark"""
        interval = 1.0 / scenario.target_tps
        end_time = time.time() + scenario.duration_seconds
        
        while time.time() < end_time:
            try:
                # Simulate consensus operation
                await asyncio.sleep(0.01 + random.uniform(0.001, 0.005))  # 10-15ms simulation
                
                # Simulate success/failure
                if random.random() > 0.95:  # 5% failure rate
                    self.error_count += 1
                else:
                    self.operation_count += 1
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                self.error_count += 1
                print(f"   Error in consensus benchmark: {e}")
    
    async def _run_p2p_load_benchmark(self, scenario: BenchmarkScenario):
        """Run P2P network load benchmark"""
        # Similar to consensus but focus on P2P message passing
        await self._run_consensus_scaling_benchmark(scenario)  # Reuse for now
    
    async def _run_post_quantum_impact_benchmark(self, scenario: BenchmarkScenario):
        """Run post-quantum impact benchmark"""
        # Test performance impact of post-quantum signatures
        await self._run_consensus_scaling_benchmark(scenario)  # Reuse for now
    
    async def _run_multi_region_latency_benchmark(self, scenario: BenchmarkScenario):
        """Run multi-region latency benchmark"""
        # Simulate multi-region operations
        interval = 1.0 / scenario.target_tps
        end_time = time.time() + scenario.duration_seconds
        
        while time.time() < end_time:
            try:
                # Simulate cross-region operation
                await asyncio.sleep(scenario.network_latency_ms / 1000.0)
                self.operation_count += 1
                await asyncio.sleep(interval)
            except Exception:
                self.error_count += 1
    
    async def _run_byzantine_fault_tolerance_benchmark(self, scenario: BenchmarkScenario):
        """Run Byzantine fault tolerance benchmark"""
        # Test consensus with Byzantine nodes
        await self._run_consensus_scaling_benchmark(scenario)  # Reuse for now
    
    async def _run_throughput_stress_benchmark(self, scenario: BenchmarkScenario):
        """Run throughput stress benchmark"""
        # Maximum throughput test
        await self._run_consensus_scaling_benchmark(scenario)  # Reuse for now
    
    def save_results_to_csv(self, filename: Optional[str] = None) -> str:
        """Save benchmark results to CSV file"""
        if filename is None:
            filename = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        filepath = self.output_directory / filename
        
        with open(filepath, 'w', newline='') as csvfile:
            if not self.benchmark_results:
                return str(filepath)
            
            # Get all field names from the first result
            fieldnames = [
                'scenario_name', 'start_time', 'duration_seconds', 'total_operations',
                'operations_per_second', 'mean_operation_time_ms', 'p95_operation_time_ms',
                'message_success_rate', 'consensus_success_rate'
            ]
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in self.benchmark_results:
                row = {
                    'scenario_name': result.scenario_name,
                    'start_time': result.start_time.isoformat(),
                    'duration_seconds': result.duration_seconds,
                    'total_operations': result.total_operations,
                    'operations_per_second': result.operations_per_second,
                    'mean_operation_time_ms': result.mean_operation_time_ms,
                    'p95_operation_time_ms': result.p95_operation_time_ms,
                    'message_success_rate': result.message_success_rate,
                    'consensus_success_rate': result.consensus_success_rate
                }
                writer.writerow(row)
        
        return str(filepath)
    
    def save_results_to_json(self, filename: Optional[str] = None) -> str:
        """Save benchmark results to JSON file"""
        if filename is None:
            filename = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        filepath = self.output_directory / filename
        
        # Convert results to JSON-serializable format
        results_data = []
        for result in self.benchmark_results:
            result_dict = {
                'scenario_name': result.scenario_name,
                'start_time': result.start_time.isoformat(),
                'end_time': result.end_time.isoformat(),
                'duration_seconds': result.duration_seconds,
                'total_operations': result.total_operations,
                'successful_operations': result.successful_operations,
                'failed_operations': result.failed_operations,
                'operations_per_second': result.operations_per_second,
                'mean_operation_time_ms': result.mean_operation_time_ms,
                'median_operation_time_ms': result.median_operation_time_ms,
                'p95_operation_time_ms': result.p95_operation_time_ms,
                'p99_operation_time_ms': result.p99_operation_time_ms,
                'std_dev_ms': result.std_dev_ms,
                'message_success_rate': result.message_success_rate,
                'consensus_success_rate': result.consensus_success_rate,
                'operation_breakdown': result.operation_breakdown,
                'metadata': result.metadata
            }
            results_data.append(result_dict)
        
        with open(filepath, 'w') as jsonfile:
            json.dump({
                'benchmark_run_timestamp': datetime.now(timezone.utc).isoformat(),
                'total_scenarios': len(self.benchmark_results),
                'results': results_data
            }, jsonfile, indent=2)
        
        return str(filepath)
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get summary of all benchmark results"""
        if not self.benchmark_results:
            return {"error": "No benchmark results available"}
        
        # Calculate aggregate statistics
        total_operations = sum(r.total_operations for r in self.benchmark_results)
        total_duration = sum(r.duration_seconds for r in self.benchmark_results)
        mean_ops_per_second = statistics.mean([r.operations_per_second for r in self.benchmark_results])
        mean_success_rate = statistics.mean([r.message_success_rate for r in self.benchmark_results])
        
        return {
            "total_scenarios_run": len(self.benchmark_results),
            "total_operations": total_operations,
            "total_test_duration": total_duration,
            "average_ops_per_second": mean_ops_per_second,
            "average_success_rate": mean_success_rate,
            "scenarios": [r.scenario_name for r in self.benchmark_results],
            "best_performing_scenario": max(self.benchmark_results, key=lambda r: r.operations_per_second).scenario_name,
            "most_reliable_scenario": max(self.benchmark_results, key=lambda r: r.message_success_rate).scenario_name
        }


# Example usage
async def run_comprehensive_benchmark_suite():
    """Run a comprehensive benchmark suite"""
    print("ðŸ“Š PRSM Comprehensive Performance Benchmark Suite")
    print("=" * 60)
    
    orchestrator = BenchmarkOrchestrator()
    scenarios = orchestrator.create_predefined_scenarios()
    
    print(f"ðŸŽ¯ Running {len(scenarios)} benchmark scenarios...")
    
    # Run a subset of scenarios for demonstration
    demo_scenarios = [s for s in scenarios if s.node_count <= 50 and s.duration_seconds <= 60]
    
    for i, scenario in enumerate(demo_scenarios[:3], 1):  # Run first 3 for demo
        print(f"\n[{i}/{len(demo_scenarios)}] Starting scenario: {scenario.name}")
        
        try:
            result = await orchestrator.run_benchmark_scenario(scenario)
            print(f"âœ… Scenario completed successfully")
        except Exception as e:
            print(f"âŒ Scenario failed: {e}")
    
    # Save results
    print(f"\nðŸ’¾ Saving results...")
    csv_file = orchestrator.save_results_to_csv()
    json_file = orchestrator.save_results_to_json()
    
    print(f"   ðŸ“„ CSV: {csv_file}")
    print(f"   ðŸ“„ JSON: {json_file}")
    
    # Display summary
    summary = orchestrator.get_performance_summary()
    print(f"\nðŸ“ˆ Performance Summary:")
    print(f"   Scenarios run: {summary['total_scenarios_run']}")
    print(f"   Total operations: {summary['total_operations']}")
    print(f"   Average ops/sec: {summary['average_ops_per_second']:.2f}")
    print(f"   Average success rate: {summary['average_success_rate']:.1%}")
    
    if summary.get('best_performing_scenario'):
        print(f"   Best performer: {summary['best_performing_scenario']}")
    
    print(f"\nâœ… Comprehensive benchmark suite completed!")


if __name__ == "__main__":
    asyncio.run(run_comprehensive_benchmark_suite())