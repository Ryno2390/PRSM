#!/usr/bin/env python3
"""
PRSM Persistent Test Environment Demo
====================================

ğŸ¯ DEMONSTRATION:
Shows how to use the persistent test environment for ongoing PRSM development
and validation. This demo creates an environment, runs comprehensive tests,
and demonstrates the monitoring capabilities.

ğŸš€ RUN THIS DEMO:
python tests/environment/demo_persistent_environment.py
"""

import asyncio
import json
from datetime import datetime, timezone
from pathlib import Path

from persistent_test_environment import (
    PersistentTestEnvironment, TestEnvironmentConfig,
    create_test_environment
)

from test_runner import TestRunner

async def demo_persistent_environment():
    """Demonstrate the persistent test environment capabilities"""
    
    print("ğŸš€ PRSM Persistent Test Environment Demo")
    print("=" * 50)
    
    # 1. Create a test environment
    print("\nğŸ“‹ Step 1: Creating Persistent Test Environment")
    
    config = TestEnvironmentConfig(
        environment_id="demo_environment",
        persistent_data=True,
        auto_cleanup=False,  # Keep environment for inspection
        performance_monitoring=True,
        health_check_interval=10  # Check every 10 seconds for demo
    )
    
    print(f"   ğŸ†” Environment ID: {config.environment_id}")
    print(f"   ğŸ“ Base Directory: {config.base_directory}")
    print(f"   âš™ï¸  Persistent Data: {config.persistent_data}")
    print(f"   ğŸ“Š Performance Monitoring: {config.performance_monitoring}")
    
    try:
        env = await create_test_environment(config)
        print(f"   âœ… Environment created successfully!")
        print(f"   ğŸ“ Environment Path: {env.env_dir}")
        
        # 2. Show environment status
        print("\nğŸ“Š Step 2: Environment Status")
        status = await env.get_environment_status()
        
        print(f"   ğŸ”„ Status: {status['status']}")
        print(f"   ğŸ“… Created: {status['created_at']}")
        print(f"   ğŸ”§ Components:")
        for component, healthy in status['components_status'].items():
            health_emoji = "âœ…" if healthy else "âŒ"
            print(f"      {health_emoji} {component.replace('_', ' ').title()}")
        
        print(f"   ğŸ“ Directories created:")
        for name, path in status['directories'].items():
            print(f"      ğŸ“‚ {name.title()}: {path}")
        
        # 3. Demonstrate test isolation
        print("\nğŸ§ª Step 3: Test Isolation Demo")
        
        async with env.test_context("demo_test_1") as test_ctx:
            print(f"   ğŸ”¬ Running isolated test: {test_ctx['test_id']}")
            print(f"   ğŸ“ Test temp directory: {test_ctx['temp_dir']}")
            
            # Create some test files
            test_file = test_ctx['temp_dir'] / "demo_data.json"
            with open(test_file, 'w') as f:
                json.dump({"demo": "data", "timestamp": datetime.now().isoformat()}, f)
            
            print(f"   ğŸ“„ Created test file: {test_file}")
            await asyncio.sleep(2)  # Simulate test work
            print(f"   âœ… Test completed - cleanup will happen automatically")
        
        print(f"   ğŸ§¹ Test context cleaned up automatically")
        
        # 4. Show test data
        print("\nğŸ’¾ Step 4: Test Data Management")
        test_data_info = status.get('test_data_info', {})
        if test_data_info:
            print(f"   ğŸ“Š Test data generated at: {test_data_info.get('generated_at', 'Unknown')}")
            print(f"   ğŸŒ± Data seed: {test_data_info.get('seed', 'Unknown')}")
            print(f"   ğŸ“ Datasets:")
            for dataset_name, dataset_info in test_data_info.get('datasets', {}).items():
                print(f"      ğŸ“‹ {dataset_name.title()}: {dataset_info.get('count', 0)} records")
        else:
            print("   â„¹ï¸  No test data info available yet")
        
        # 5. Run comprehensive tests using the test runner
        print("\nğŸ”§ Step 5: Running Comprehensive Tests")
        
        runner = TestRunner()
        runner.environments[config.environment_id] = env
        
        # Run a subset of tests for demo
        test_suites = ["system_health", "integration"]
        print(f"   ğŸ§ª Running test suites: {', '.join(test_suites)}")
        
        results = await runner.run_comprehensive_tests(config.environment_id, test_suites)
        
        print(f"   ğŸ“Š Test Results Summary:")
        summary = results.get('summary', {})
        print(f"      âœ… Passed: {summary.get('passed_suites', 0)}")
        print(f"      âŒ Failed: {summary.get('failed_suites', 0)}")
        print(f"      ğŸ“ˆ Success Rate: {summary.get('success_rate', 0):.1%}")
        print(f"      â±ï¸  Duration: {summary.get('total_duration', 0):.2f}s")
        
        # 6. Show monitoring data
        print("\nğŸ“ˆ Step 6: Performance Monitoring")
        
        if status.get('performance_metrics'):
            metrics = status['performance_metrics']
            print(f"   ğŸ’¾ Memory Usage: {metrics.get('memory_usage_mb', 0):.1f} MB")
            print(f"   ğŸ–¥ï¸  CPU Usage: {metrics.get('cpu_percent', 0):.1f}%")
            
            if 'database_response_time' in metrics:
                print(f"   ğŸ—„ï¸  Database Response: {metrics['database_response_time']:.3f}s")
            if 'redis_response_time' in metrics:
                print(f"   ğŸ”´ Redis Response: {metrics['redis_response_time']:.3f}s")
        else:
            print("   â„¹ï¸  Performance metrics will be available after monitoring period")
        
        # 7. Demonstrate environment persistence
        print("\nğŸ’¾ Step 7: Environment Persistence")
        
        print(f"   ğŸ“ Environment files saved to: {env.env_dir}")
        print(f"   ğŸ“Š Configuration: {env.config_dir / 'environment.json'}")
        print(f"   ğŸ“ Logs directory: {env.logs_dir}")
        print(f"   ğŸ’¾ Data directory: {env.data_dir}")
        
        # Show what files were created
        config_file = env.config_dir / "environment.json"
        if config_file.exists():
            with open(config_file) as f:
                env_config = json.load(f)
            print(f"   âœ… Environment config saved (created: {env_config.get('created_at', 'Unknown')})")
        
        # 8. Let monitoring run for a bit
        print("\nâ±ï¸  Step 8: Monitoring Demonstration")
        print("   ğŸ“Š Letting environment run for 30 seconds to show monitoring...")
        
        for i in range(6):  # 30 seconds in 5-second intervals
            await asyncio.sleep(5)
            current_status = await env.get_environment_status()
            healthy_count = sum(1 for healthy in current_status['components_status'].values() if healthy)
            total_count = len(current_status['components_status'])
            print(f"   ğŸ“ˆ Health check {i+1}/6: {healthy_count}/{total_count} components healthy")
        
        # 9. Show how to stop the environment
        print("\nğŸ›‘ Step 9: Stopping Environment")
        print("   âš ï¸  Note: Environment will be stopped but data preserved (auto_cleanup=False)")
        
        await env.stop()
        final_status = await env.get_environment_status()
        print(f"   âœ… Environment stopped: {final_status['status']}")
        
        # 10. Summary
        print("\nğŸ“‹ Step 10: Demo Summary")
        print("   ğŸ‰ Persistent Test Environment Demo Completed!")
        print(f"   ğŸ“ Environment preserved at: {env.env_dir}")
        print("   ğŸ’¡ Key Benefits Demonstrated:")
        print("      âœ… Persistent state across test runs")
        print("      âœ… Isolated test execution with cleanup")
        print("      âœ… Comprehensive health monitoring")
        print("      âœ… Performance metrics collection")
        print("      âœ… Test data management")
        print("      âœ… Automated service orchestration")
        
        print("\nğŸ”„ Next Steps:")
        print("   1. Inspect the environment directory")
        print("   2. Run individual test suites")
        print("   3. Use the environment for development")
        print("   4. Extend with custom test scenarios")
        
        return env.config.environment_id
        
    except Exception as e:
        print(f"   âŒ Demo failed: {str(e)}")
        raise

async def cleanup_demo_environment(environment_id: str):
    """Clean up the demo environment"""
    print(f"\nğŸ§¹ Cleaning up demo environment: {environment_id}")
    
    import shutil
    env_dir = Path.cwd() / "test_environments" / environment_id
    
    if env_dir.exists():
        shutil.rmtree(env_dir, ignore_errors=True)
        print(f"   âœ… Environment directory removed: {env_dir}")
    else:
        print(f"   â„¹ï¸  Environment directory not found: {env_dir}")

async def main():
    """Main demo function"""
    print("ğŸ¯ Starting PRSM Persistent Test Environment Demo")
    print("   This will demonstrate all key features of the persistent test environment")
    print("   including environment creation, test execution, and monitoring.\n")
    
    try:
        environment_id = await demo_persistent_environment()
        
        print(f"\nğŸ‰ Demo completed successfully!")
        print(f"ğŸ“ Environment preserved at: test_environments/{environment_id}")
        
        # Ask if user wants to clean up
        response = input("\nğŸ—‘ï¸  Clean up demo environment? (y/N): ").strip().lower()
        if response == 'y':
            await cleanup_demo_environment(environment_id)
        else:
            print(f"ğŸ“ Environment preserved for inspection: test_environments/{environment_id}")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  Demo cancelled by user")
    except Exception as e:
        print(f"\nâŒ Demo failed with error: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())