#!/usr/bin/env python3
"""
Security Features Test Runner
============================

Quick validation script to test the enhanced security features
before uploading to GitHub. Runs essential security component tests.
"""

import asyncio
import sys
import tempfile
from pathlib import Path
from datetime import datetime


def print_header(title: str):
    """Print a formatted test section header"""
    print(f"\n{'=' * 60}")
    print(f"  {title}")
    print(f"{'=' * 60}")


def print_result(test_name: str, passed: bool, details: str = ""):
    """Print formatted test result"""
    status = "‚úÖ PASS" if passed else "‚ùå FAIL"
    print(f"{status} {test_name}")
    if details:
        print(f"     {details}")


async def test_security_imports():
    """Test that all security components can be imported"""
    print_header("Security Component Import Tests")
    
    try:
        from prsm.integrations.security import (
            security_orchestrator, audit_logger, threat_detector,
            enhanced_sandbox_manager, SecurityAssessment
        )
        print_result("Import security orchestrator", True)
        
        from prsm.integrations.security.threat_detector import ThreatLevel
        print_result("Import threat detector components", True)
        
        from prsm.integrations.security.audit_logger import EventLevel
        print_result("Import audit logger components", True)
        
        from prsm.integrations.api.security_api import security_router
        print_result("Import security API router", True)
        
        return True
        
    except Exception as e:
        print_result("Security imports", False, str(e))
        return False


async def test_threat_detection():
    """Test basic threat detection functionality"""
    print_header("Threat Detection Tests")
    
    try:
        from prsm.integrations.security.threat_detector import ThreatDetector, ThreatLevel
        
        detector = ThreatDetector()
        print_result("Initialize threat detector", True)
        
        # Create test content
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write("""
# Safe test content
print("Hello, PRSM!")

def safe_function():
    return "This is safe"
""")
            safe_file = f.name
        
        # Test safe content
        result = await detector.scan_threats(safe_file, {"name": "safe_test"})
        safe_passed = result.threat_level in [ThreatLevel.NONE, ThreatLevel.LOW]
        print_result("Safe content detection", safe_passed, f"Threat level: {result.threat_level.value}")
        
        # Create malicious content
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write("""
# Malicious test content
import subprocess
eval(base64.b64decode("malicious_code"))
subprocess.system("rm -rf /")
""")
            malicious_file = f.name
        
        # Test malicious content
        result = await detector.scan_threats(malicious_file, {"name": "malicious_test"})
        malicious_detected = result.threat_level in [ThreatLevel.MEDIUM, ThreatLevel.HIGH, ThreatLevel.CRITICAL]
        print_result("Malicious content detection", malicious_detected, f"Threats found: {len(result.threats)}")
        
        # Cleanup
        Path(safe_file).unlink()
        Path(malicious_file).unlink()
        
        return safe_passed and malicious_detected
        
    except Exception as e:
        print_result("Threat detection tests", False, str(e))
        return False


async def test_security_orchestrator():
    """Test security orchestrator functionality"""
    print_header("Security Orchestrator Tests")
    
    try:
        from prsm.integrations.security.security_orchestrator import security_orchestrator
        
        # Create test content
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write("""
#!/usr/bin/env python3
# MIT License test file

def calculator_add(a, b):
    return a + b

if __name__ == "__main__":
    print(f"2 + 3 = {calculator_add(2, 3)}")
""")
            test_file = f.name
        
        metadata = {
            "name": "test-calculator",
            "description": "A simple calculator for testing",
            "license": {"key": "mit", "name": "MIT License"},
            "language": "Python"
        }
        
        # Run comprehensive assessment
        assessment = await security_orchestrator.comprehensive_security_assessment(
            content_path=test_file,
            metadata=metadata,
            user_id="test_user",
            platform="github",
            content_id="test-calculator",
            enable_sandbox=False  # Disable for quick test
        )
        
        print_result("Security assessment creation", True, f"Assessment ID: {assessment.assessment_id[:8]}...")
        print_result("Security scans completion", len(assessment.scans_completed) >= 3, 
                    f"Completed: {', '.join(assessment.scans_completed)}")
        print_result("Safe content approval", assessment.security_passed, 
                    f"Risk level: {assessment.overall_risk_level.value}")
        
        # Cleanup
        Path(test_file).unlink()
        
        return assessment.security_passed and len(assessment.scans_completed) >= 3
        
    except Exception as e:
        print_result("Security orchestrator tests", False, str(e))
        return False


async def test_audit_logging():
    """Test audit logging functionality"""
    print_header("Audit Logging Tests")
    
    try:
        from prsm.integrations.security.audit_logger import AuditLogger, SecurityEvent, EventLevel
        
        with tempfile.TemporaryDirectory() as temp_dir:
            logger = AuditLogger(log_dir=temp_dir)
            print_result("Initialize audit logger", True)
            
            # Create test event
            event = SecurityEvent(
                event_type="test_security_event",
                level=EventLevel.INFO,
                user_id="test_user",
                platform="github",
                description="Test security event for validation"
            )
            
            # Log event
            logger.log_event(event)
            print_result("Log security event", True)
            
            # Verify event was logged
            recent_events = logger.get_recent_events(limit=5)
            event_found = any(e['event_type'] == 'test_security_event' for e in recent_events)
            print_result("Retrieve logged events", event_found, f"Found {len(recent_events)} recent events")
            
            # Get statistics
            stats = logger.get_security_stats()
            stats_valid = 'total_events' in stats and stats['total_events'] > 0
            print_result("Security statistics", stats_valid, f"Total events: {stats['total_events']}")
            
            return event_found and stats_valid
            
    except Exception as e:
        print_result("Audit logging tests", False, str(e))
        return False


async def test_api_integration():
    """Test API integration"""
    print_header("API Integration Tests")
    
    try:
        from prsm.integrations.api.security_api import security_router
        from fastapi import FastAPI
        
        # Test that security router can be included
        app = FastAPI()
        app.include_router(security_router, prefix="/integrations/security")
        print_result("Security API router integration", True)
        
        # Test API client mock responses
        from PRSM_ui_mockup.js import api_client  # This would fail, but we can check structure
        
        # For now, just verify the router was created successfully
        routes = [route.path for route in security_router.routes]
        expected_routes = ['/scan', '/events/recent', '/stats', '/health', '/policies']
        
        routes_present = all(any(expected in route for route in routes) for expected in expected_routes)
        print_result("API endpoints availability", routes_present, f"Routes: {len(routes)}")
        
        return routes_present
        
    except Exception as e:
        print_result("API integration tests", False, str(e))
        return False


async def test_configuration_integration():
    """Test configuration management integration"""
    print_header("Configuration Integration Tests")
    
    try:
        from prsm.integrations.config.credential_manager import credential_manager
        from prsm.integrations.config.integration_config import config_manager
        
        print_result("Import configuration managers", True)
        
        # Test credential manager
        stats = credential_manager.get_storage_stats()
        cred_manager_working = isinstance(stats, dict)
        print_result("Credential manager functionality", cred_manager_working)
        
        # Test config manager  
        system_stats = config_manager.get_system_stats()
        config_manager_working = isinstance(system_stats, dict)
        print_result("Configuration manager functionality", config_manager_working)
        
        return cred_manager_working and config_manager_working
        
    except Exception as e:
        print_result("Configuration integration tests", False, str(e))
        return False


async def run_all_tests():
    """Run all validation tests"""
    print("üîí PRSM Enhanced Security Features Validation")
    print(f"‚è∞ Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    test_results = []
    
    # Run all test suites
    test_results.append(await test_security_imports())
    test_results.append(await test_threat_detection())
    test_results.append(await test_security_orchestrator())
    test_results.append(await test_audit_logging())
    test_results.append(await test_api_integration())
    test_results.append(await test_configuration_integration())
    
    # Summary
    print_header("Test Summary")
    passed_count = sum(test_results)
    total_count = len(test_results)
    
    print(f"Tests Passed: {passed_count}/{total_count}")
    
    if passed_count == total_count:
        print("\nüéâ All security features validated successfully!")
        print("‚úÖ Ready for GitHub upload")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {total_count - passed_count} test(s) failed")
        print("‚ùå Please review failures before uploading")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(run_all_tests())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• Unexpected error during testing: {e}")
        sys.exit(1)