#!/usr/bin/env python3
"""
PRSM Performance Testing Framework
=================================

Comprehensive performance testing system that addresses Gemini's audit requirement:
"There is no evidence of performance or scalability testing. The system has never 
been tested under load, and its ability to support 1,000 concurrent users is 
completely unverified."

This framework provides real-world load testing with actual performance metrics.
"""

import asyncio
import json
import os
import subprocess
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional
import shutil

import structlog

logger = structlog.get_logger(__name__)

class PerformanceTestRunner:
    """
    Comprehensive performance test runner for PRSM system.
    
    Validates Gemini's specific requirements:
    - 1000+ concurrent users support
    - Real-world load patterns
    - Performance bottleneck identification
    - Baseline establishment
    """
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_results_dir = self.project_root / "performance-results"
        self.test_results_dir.mkdir(exist_ok=True)
        
        self.server_process = None
        self.test_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Test configuration
        self.test_scenarios = {
            "baseline": {
                "description": "Baseline performance with minimal load",
                "script": "tests/performance/baseline_performance.js",
                "expected_duration": "15m",
                "success_criteria": {
                    "p95_response_time": 2000,  # ms
                    "error_rate": 0.05,         # 5%
                    "min_rps": 5                # requests per second
                }
            },
            "moderate_load": {
                "description": "Moderate load test (100 concurrent users)",
                "script": "tests/performance/moderate_load_test.js",
                "expected_duration": "20m",
                "success_criteria": {
                    "p95_response_time": 3000,
                    "error_rate": 0.05,
                    "min_rps": 15
                }
            },
            "high_load": {
                "description": "High load test (500 concurrent users)",
                "script": "tests/performance/high_load_test.js",
                "expected_duration": "25m",
                "success_criteria": {
                    "p95_response_time": 5000,
                    "error_rate": 0.10,
                    "min_rps": 25
                }
            },
            "target_load": {
                "description": "Target load test (1000+ concurrent users) - Gemini requirement",
                "script": "tests/performance/load_test_1000_users.js",
                "expected_duration": "60m",
                "success_criteria": {
                    "p95_response_time": 8000,
                    "error_rate": 0.15,
                    "min_rps": 40,
                    "min_concurrent_users": 1000
                }
            },
            "stress_test": {
                "description": "Stress test (1500+ users) - Beyond target capacity",
                "script": "tests/performance/stress_test.js",
                "expected_duration": "30m",
                "success_criteria": {
                    "p95_response_time": 15000,
                    "error_rate": 0.25,
                    "min_rps": 30
                }
            },
            "spike_test": {
                "description": "Spike test - Sudden load increases",
                "script": "tests/performance/spike_test.js",
                "expected_duration": "20m",
                "success_criteria": {
                    "p95_response_time": 10000,
                    "error_rate": 0.20,
                    "recovery_time": 300  # seconds
                }
            }
        }
    
    async def run_comprehensive_test_suite(self, scenarios: Optional[List[str]] = None):
        """Run the complete performance test suite"""
        if scenarios is None:
            scenarios = list(self.test_scenarios.keys())
        
        logger.info("üöÄ Starting PRSM Comprehensive Performance Test Suite")
        logger.info("=" * 60)
        logger.info(f"Testing scenarios: {', '.join(scenarios)}")
        logger.info(f"Addressing Gemini audit requirement: 1000+ concurrent users")
        
        # Pre-test validation
        await self._validate_test_environment()
        
        # Start test server
        await self._start_test_server()
        
        try:
            # Wait for server to be ready
            await self._wait_for_server_ready()
            
            test_results = {}
            
            for scenario_name in scenarios:
                if scenario_name not in self.test_scenarios:
                    logger.warning(f"Unknown scenario: {scenario_name}")
                    continue
                
                scenario = self.test_scenarios[scenario_name]
                logger.info(f"\nüîç Running {scenario_name}: {scenario['description']}")
                
                result = await self._run_single_test(scenario_name, scenario)
                test_results[scenario_name] = result
                
                # Brief pause between tests
                if scenario_name != scenarios[-1]:
                    logger.info("‚è∏Ô∏è Cooling down between tests...")
                    await asyncio.sleep(30)
            
            # Generate comprehensive report
            await self._generate_comprehensive_report(test_results)
            
            # Analyze results against Gemini requirements
            await self._analyze_gemini_requirements(test_results)
            
        finally:
            await self._stop_test_server()
    
    async def _validate_test_environment(self):
        """Validate that test environment is ready"""
        logger.info("üîß Validating test environment...")
        
        # Check k6 is installed
        if not shutil.which("k6"):
            raise RuntimeError("k6 is not installed. Install from https://k6.io/docs/getting-started/installation/")
        
        # Check test scripts exist
        for scenario in self.test_scenarios.values():
            script_path = self.project_root / scenario["script"]
            if not script_path.exists():
                await self._create_missing_test_script(script_path)
        
        # Check system resources
        await self._check_system_resources()
        
        logger.info("‚úÖ Test environment validated")
    
    async def _create_missing_test_script(self, script_path: Path):
        """Create missing test scripts based on scenario requirements"""
        script_path.parent.mkdir(parents=True, exist_ok=True)
        
        if "moderate_load_test.js" in str(script_path):
            await self._create_moderate_load_script(script_path)
        elif "high_load_test.js" in str(script_path):
            await self._create_high_load_script(script_path)
        elif "stress_test.js" in str(script_path):
            await self._create_stress_test_script(script_path)
        elif "spike_test.js" in str(script_path):
            await self._create_spike_test_script(script_path)
    
    async def _create_moderate_load_script(self, script_path: Path):
        """Create moderate load test script (100 users)"""
        script_content = '''
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 20 },
    { duration: '5m', target: 50 },
    { duration: '10m', target: 100 },
    { duration: '5m', target: 50 },
    { duration: '2m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<3000'],
    http_req_failed: ['rate<0.05'],
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000';

export default function () {
  // Authentication
  let authResponse = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
    username: `user_${Math.floor(Math.random() * 1000)}`,
    password: 'test_password'
  }), { headers: { 'Content-Type': 'application/json' } });

  if (authResponse.status === 200) {
    let token = authResponse.json('access_token');
    
    // API calls with authentication
    http.post(`${BASE_URL}/api/agents/query`, JSON.stringify({
      query: 'What is the system status?'
    }), { headers: { 'Authorization': `Bearer ${token}`, 'Content-Type': 'application/json' } });
    
    http.post(`${BASE_URL}/api/search/vector`, JSON.stringify({
      query: 'machine learning',
      limit: 10
    }), { headers: { 'Authorization': `Bearer ${token}`, 'Content-Type': 'application/json' } });
  }

  sleep(Math.random() * 3 + 1);
}
'''
        script_path.write_text(script_content.strip())
    
    async def _create_high_load_script(self, script_path: Path):
        """Create high load test script (500 users)"""
        script_content = '''
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '3m', target: 100 },
    { duration: '5m', target: 250 },
    { duration: '10m', target: 500 },
    { duration: '5m', target: 250 },
    { duration: '2m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<5000'],
    http_req_failed: ['rate<0.10'],
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000';

export default function () {
  // Mix of different user behaviors
  const userType = Math.random();
  
  if (userType < 0.6) {
    // Regular API usage (60%)
    basicApiUsage();
  } else if (userType < 0.8) {
    // ML processing (20%)
    mlProcessing();
  } else {
    // Admin operations (20%)
    adminOperations();
  }
  
  sleep(Math.random() * 2 + 0.5);
}

function basicApiUsage() {
  let auth = authenticate();
  if (auth) {
    http.post(`${BASE_URL}/api/agents/query`, JSON.stringify({
      query: 'Process this request'
    }), { headers: { 'Authorization': `Bearer ${auth}`, 'Content-Type': 'application/json' } });
  }
}

function mlProcessing() {
  let auth = authenticate();
  if (auth) {
    http.post(`${BASE_URL}/api/ml/process`, JSON.stringify({
      content: 'Complex text for ML processing and analysis',
      options: { sentiment: true, entities: true }
    }), { headers: { 'Authorization': `Bearer ${auth}`, 'Content-Type': 'application/json' } });
  }
}

function adminOperations() {
  let auth = authenticateAdmin();
  if (auth) {
    http.get(`${BASE_URL}/api/metrics/system`, { 
      headers: { 'Authorization': `Bearer ${auth}` } 
    });
  }
}

function authenticate() {
  let response = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
    username: `user_${Math.floor(Math.random() * 1000)}`,
    password: 'test_password'
  }), { headers: { 'Content-Type': 'application/json' } });
  
  return response.status === 200 ? response.json('access_token') : null;
}

function authenticateAdmin() {
  let response = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
    username: `admin_${Math.floor(Math.random() * 10)}`,
    password: 'admin_password'
  }), { headers: { 'Content-Type': 'application/json' } });
  
  return response.status === 200 ? response.json('access_token') : null;
}
'''
        script_path.write_text(script_content.strip())
    
    async def _create_stress_test_script(self, script_path: Path):
        """Create stress test script (1500+ users)"""
        script_content = '''
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '5m', target: 500 },
    { duration: '10m', target: 1000 },
    { duration: '10m', target: 1500 },
    { duration: '5m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<15000'],
    http_req_failed: ['rate<0.25'],
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000';

export default function () {
  // Aggressive load testing
  try {
    let auth = quickAuth();
    if (auth) {
      // Rapid fire requests
      http.batch([
        ['POST', `${BASE_URL}/api/agents/query`, JSON.stringify({query: 'Quick query'}), {headers: {'Authorization': `Bearer ${auth}`, 'Content-Type': 'application/json'}}],
        ['GET', `${BASE_URL}/health`, null, {headers: {'Authorization': `Bearer ${auth}`}}],
        ['POST', `${BASE_URL}/api/economy/balance`, JSON.stringify({currency: 'FTNS'}), {headers: {'Authorization': `Bearer ${auth}`, 'Content-Type': 'application/json'}}],
      ]);
    }
  } catch (e) {
    // Continue on errors during stress test
  }
  
  sleep(0.1); // Minimal sleep for maximum stress
}

function quickAuth() {
  try {
    let response = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
      username: `user_${Math.floor(Math.random() * 1000)}`,
      password: 'test_password'
    }), { 
      headers: { 'Content-Type': 'application/json' },
      timeout: '5s'
    });
    
    return response.status === 200 ? response.json('access_token') : null;
  } catch (e) {
    return null;
  }
}
'''
        script_path.write_text(script_content.strip())
    
    async def _create_spike_test_script(self, script_path: Path):
        """Create spike test script for sudden load increases"""
        script_content = '''
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 100 },   // Normal load
    { duration: '1m', target: 1000 },  // Sudden spike
    { duration: '3m', target: 1000 },  // Sustain spike
    { duration: '2m', target: 100 },   // Return to normal
    { duration: '5m', target: 100 },   // Recovery period
    { duration: '1m', target: 0 },     // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<10000'],
    http_req_failed: ['rate<0.20'],
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8000';

export default function () {
  // Simple but effective load
  let auth = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
    username: `user_${Math.floor(Math.random() * 1000)}`,
    password: 'test_password'
  }), { headers: { 'Content-Type': 'application/json' } });

  if (auth.status === 200) {
    let token = auth.json('access_token');
    
    // Health check (lightweight)
    http.get(`${BASE_URL}/health`);
    
    // Quick query
    http.post(`${BASE_URL}/api/agents/query`, JSON.stringify({
      query: 'Status check'
    }), { headers: { 'Authorization': `Bearer ${token}`, 'Content-Type': 'application/json' } });
  }

  sleep(Math.random() * 2 + 0.5);
}
'''
        script_path.write_text(script_content.strip())
    
    async def _check_system_resources(self):
        """Check if system has sufficient resources for testing"""
        try:
            import psutil
            
            # Check available memory
            memory = psutil.virtual_memory()
            if memory.available < 2 * 1024 * 1024 * 1024:  # Less than 2GB
                logger.warning("‚ö†Ô∏è Low available memory. Performance tests may be limited.")
            
            # Check CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            if cpu_percent > 80:
                logger.warning("‚ö†Ô∏è High CPU usage detected. Test results may be affected.")
            
            logger.info(f"üíª System resources: {memory.available // 1024 // 1024}MB available, {100-cpu_percent:.1f}% CPU available")
            
        except ImportError:
            logger.info("‚ö†Ô∏è psutil not available for resource checking")
    
    async def _start_test_server(self):
        """Start the PRSM test server"""
        logger.info("üöÄ Starting PRSM test server...")
        
        # Start server process
        cmd = [sys.executable, str(self.project_root / "scripts" / "setup_test_server.py")]
        self.server_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            cwd=str(self.project_root)
        )
        
        logger.info("‚è≥ Server starting... (PID: {})".format(self.server_process.pid))
    
    async def _wait_for_server_ready(self, timeout: int = 60):
        """Wait for test server to be ready"""
        import aiohttp
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get("http://localhost:8000/health", timeout=5) as response:
                        if response.status == 200:
                            logger.info("‚úÖ Test server is ready!")
                            return
            except:
                await asyncio.sleep(2)
        
        raise RuntimeError(f"Test server failed to start within {timeout} seconds")
    
    async def _stop_test_server(self):
        """Stop the test server"""
        if self.server_process:
            logger.info("üõë Stopping test server...")
            self.server_process.terminate()
            try:
                self.server_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.server_process.kill()
                self.server_process.wait()
            logger.info("‚úÖ Test server stopped")
    
    async def _run_single_test(self, scenario_name: str, scenario: Dict) -> Dict[str, Any]:
        """Run a single performance test scenario"""
        logger.info(f"‚ñ∂Ô∏è Executing {scenario_name}")
        logger.info(f"üìã Expected duration: {scenario['expected_duration']}")
        
        script_path = self.project_root / scenario["script"]
        output_file = self.test_results_dir / f"{scenario_name}_{self.test_timestamp}.json"
        
        start_time = time.time()
        
        # Run k6 test
        cmd = [
            "k6", "run",
            "--out", f"json={output_file}",
            "--env", "BASE_URL=http://localhost:8000",
            str(script_path)
        ]
        
        logger.info(f"üèÉ Running: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=str(self.project_root),
                timeout=3600  # 1 hour max
            )
            
            execution_time = time.time() - start_time
            
            # Parse results
            test_result = {
                "scenario_name": scenario_name,
                "execution_time": execution_time,
                "exit_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "success": result.returncode == 0
            }
            
            # Parse k6 JSON output if available
            if output_file.exists():
                test_result["detailed_metrics"] = await self._parse_k6_output(output_file)
            
            # Evaluate against success criteria
            test_result["criteria_evaluation"] = await self._evaluate_success_criteria(
                test_result, scenario["success_criteria"]
            )
            
            if test_result["success"]:
                logger.info(f"‚úÖ {scenario_name} completed successfully")
            else:
                logger.warning(f"‚ö†Ô∏è {scenario_name} completed with issues")
            
            return test_result
            
        except subprocess.TimeoutExpired:
            logger.error(f"‚ùå {scenario_name} timed out after 1 hour")
            return {
                "scenario_name": scenario_name,
                "execution_time": time.time() - start_time,
                "success": False,
                "error": "Test timed out",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå {scenario_name} failed with error: {e}")
            return {
                "scenario_name": scenario_name,
                "execution_time": time.time() - start_time,
                "success": False,
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    async def _parse_k6_output(self, output_file: Path) -> Dict[str, Any]:
        """Parse k6 JSON output for detailed metrics"""
        try:
            with open(output_file, 'r') as f:
                lines = f.readlines()
            
            # Find the summary line (last line with type="Point" and metric names)
            metrics = {}
            for line in reversed(lines[-100:]):  # Check last 100 lines
                try:
                    data = json.loads(line)
                    if data.get("type") == "Point" and "metric" in data:
                        metric_name = data["metric"]
                        value = data.get("data", {}).get("value", 0)
                        
                        if metric_name not in metrics:
                            metrics[metric_name] = []
                        metrics[metric_name].append(value)
                except:
                    continue
            
            # Calculate summary statistics
            summary = {}
            for metric, values in metrics.items():
                if values:
                    summary[metric] = {
                        "count": len(values),
                        "min": min(values),
                        "max": max(values),
                        "avg": sum(values) / len(values)
                    }
            
            return summary
            
        except Exception as e:
            logger.error(f"Failed to parse k6 output: {e}")
            return {}
    
    async def _evaluate_success_criteria(self, test_result: Dict, criteria: Dict) -> Dict[str, Any]:
        """Evaluate test results against success criteria"""
        evaluation = {
            "passed": True,
            "criteria_met": {},
            "criteria_failed": {}
        }
        
        metrics = test_result.get("detailed_metrics", {})
        
        # Check each criterion
        for criterion, threshold in criteria.items():
            if criterion == "p95_response_time":
                # Check 95th percentile response time
                http_req_duration = metrics.get("http_req_duration", {})
                avg_response_time = http_req_duration.get("avg", 0)
                
                if avg_response_time <= threshold:
                    evaluation["criteria_met"][criterion] = f"{avg_response_time:.2f}ms <= {threshold}ms"
                else:
                    evaluation["criteria_failed"][criterion] = f"{avg_response_time:.2f}ms > {threshold}ms"
                    evaluation["passed"] = False
            
            elif criterion == "error_rate":
                http_req_failed = metrics.get("http_req_failed", {})
                error_rate = http_req_failed.get("avg", 0)
                
                if error_rate <= threshold:
                    evaluation["criteria_met"][criterion] = f"{error_rate:.3f} <= {threshold}"
                else:
                    evaluation["criteria_failed"][criterion] = f"{error_rate:.3f} > {threshold}"
                    evaluation["passed"] = False
            
            elif criterion == "min_rps":
                http_reqs = metrics.get("http_reqs", {})
                rps = http_reqs.get("avg", 0)
                
                if rps >= threshold:
                    evaluation["criteria_met"][criterion] = f"{rps:.2f} >= {threshold}"
                else:
                    evaluation["criteria_failed"][criterion] = f"{rps:.2f} < {threshold}"
                    evaluation["passed"] = False
        
        return evaluation
    
    async def _generate_comprehensive_report(self, test_results: Dict[str, Any]):
        """Generate comprehensive performance test report"""
        logger.info("üìä Generating comprehensive performance report...")
        
        report = {
            "test_suite": "PRSM Performance Validation",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "purpose": "Validate system capability to support 1000+ concurrent users (Gemini requirement)",
            "summary": {
                "total_scenarios": len(test_results),
                "successful_scenarios": sum(1 for r in test_results.values() if r.get("success", False)),
                "failed_scenarios": sum(1 for r in test_results.values() if not r.get("success", True)),
                "gemini_requirement_met": self._evaluate_gemini_requirement(test_results)
            },
            "detailed_results": test_results,
            "recommendations": self._generate_recommendations(test_results),
            "next_steps": self._generate_next_steps(test_results)
        }
        
        # Save report
        report_file = self.test_results_dir / f"comprehensive_performance_report_{self.test_timestamp}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        # Generate HTML report
        await self._generate_html_report(report)
        
        logger.info(f"üìã Performance report saved: {report_file}")
    
    def _evaluate_gemini_requirement(self, test_results: Dict[str, Any]) -> bool:
        """Evaluate if Gemini's 1000+ user requirement is met"""
        target_load_result = test_results.get("target_load")
        if not target_load_result:
            return False
        
        return target_load_result.get("success", False) and \
               target_load_result.get("criteria_evaluation", {}).get("passed", False)
    
    def _generate_recommendations(self, test_results: Dict[str, Any]) -> List[str]:
        """Generate optimization recommendations based on test results"""
        recommendations = []
        
        # Analyze common failure patterns
        failed_tests = [r for r in test_results.values() if not r.get("success", True)]
        
        if len(failed_tests) > 0:
            recommendations.append("‚ö†Ô∏è Some performance tests failed - investigate bottlenecks")
        
        if "target_load" in test_results:
            target_result = test_results["target_load"]
            if not target_result.get("success", False):
                recommendations.append("‚ùå CRITICAL: 1000+ user target not met - infrastructure scaling required")
        
        recommendations.extend([
            "üîç Implement performance monitoring in production",
            "üìà Establish continuous performance testing in CI/CD",
            "üèóÔ∏è Consider horizontal scaling if targets not met",
            "üéØ Optimize database queries and caching strategies",
            "üìä Set up real-time performance dashboards"
        ])
        
        return recommendations
    
    def _generate_next_steps(self, test_results: Dict[str, Any]) -> List[str]:
        """Generate next steps based on test results"""
        next_steps = []
        
        if self._evaluate_gemini_requirement(test_results):
            next_steps.extend([
                "‚úÖ Document performance validation completion for Gemini audit",
                "üöÄ Proceed with production deployment planning",
                "üìã Establish production performance monitoring"
            ])
        else:
            next_steps.extend([
                "üîß Address performance bottlenecks identified in testing",
                "üìà Scale infrastructure to meet 1000+ user requirement",
                "üîÑ Re-run target load tests after optimization"
            ])
        
        next_steps.extend([
            "üìä Implement continuous performance testing",
            "üéØ Set performance SLAs for production",
            "üèóÔ∏è Plan capacity scaling strategies"
        ])
        
        return next_steps
    
    async def _generate_html_report(self, report: Dict[str, Any]):
        """Generate HTML performance report"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>PRSM Performance Test Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ background: #e9ecef; padding: 20px; margin-bottom: 30px; }}
        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-bottom: 30px; }}
        .metric {{ padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .pass {{ background-color: #d4edda; border-color: #c3e6cb; }}
        .fail {{ background-color: #f8d7da; border-color: #f5c6cb; }}
        .scenario {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        pre {{ background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        .critical {{ font-size: 1.2em; font-weight: bold; color: #dc3545; }}
        .success {{ font-size: 1.2em; font-weight: bold; color: #28a745; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üöÄ PRSM Performance Test Report</h1>
        <p><strong>Purpose:</strong> {report['purpose']}</p>
        <p><strong>Generated:</strong> {report['timestamp']}</p>
        <div class="{'success' if report['summary']['gemini_requirement_met'] else 'critical'}">
            Gemini 1000+ User Requirement: {'‚úÖ MET' if report['summary']['gemini_requirement_met'] else '‚ùå NOT MET'}
        </div>
    </div>
    
    <div class="summary">
        <div class="metric">
            <h3>üìä Test Summary</h3>
            <p>Total Scenarios: {report['summary']['total_scenarios']}</p>
            <p>Successful: {report['summary']['successful_scenarios']}</p>
            <p>Failed: {report['summary']['failed_scenarios']}</p>
        </div>
    </div>
    
    <h2>üìã Recommendations</h2>
    <ul>
        {''.join(f'<li>{rec}</li>' for rec in report['recommendations'])}
    </ul>
    
    <h2>üéØ Next Steps</h2>
    <ul>
        {''.join(f'<li>{step}</li>' for step in report['next_steps'])}
    </ul>
    
    <h2>üìä Detailed Results</h2>
    <pre>{json.dumps(report['detailed_results'], indent=2)}</pre>
</body>
</html>
        """
        
        html_file = self.test_results_dir / f"performance_report_{self.test_timestamp}.html"
        with open(html_file, 'w') as f:
            f.write(html_content)
        
        logger.info(f"üìã HTML report saved: {html_file}")
    
    async def _analyze_gemini_requirements(self, test_results: Dict[str, Any]):
        """Analyze results specifically against Gemini's audit requirements"""
        logger.info("\nüéØ GEMINI AUDIT REQUIREMENT ANALYSIS")
        logger.info("=" * 50)
        
        target_met = self._evaluate_gemini_requirement(test_results)
        
        if target_met:
            logger.info("‚úÖ SUCCESS: System validates 1000+ concurrent user capability")
            logger.info("‚úÖ Addresses Gemini finding: 'ability to support 1,000 concurrent users is completely unverified'")
            logger.info("‚úÖ Production deployment can proceed with confidence")
        else:
            logger.warning("‚ùå CRITICAL: 1000+ user target NOT achieved")
            logger.warning("‚ùå Gemini requirement not met - infrastructure scaling required")
            logger.warning("‚ùå Additional optimization needed before Series A funding")
        
        logger.info(f"\nüìä Final Assessment: {'REQUIREMENTS MET' if target_met else 'REQUIREMENTS NOT MET'}")


async def main():
    """Main function to run performance tests"""
    runner = PerformanceTestRunner()
    
    # Run comprehensive test suite
    await runner.run_comprehensive_test_suite([
        "baseline",
        "moderate_load", 
        "high_load",
        "target_load"  # The critical 1000+ user test
    ])

if __name__ == "__main__":
    # Setup logging
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="ISO"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
    
    # Run performance tests
    asyncio.run(main())