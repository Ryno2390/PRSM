{
  "execution_id": "3372aa3a-1a6b-40a5-9417-44dcec65827d",
  "status": "running",
  "started_at": "2025-07-21T13:36:14.937179+00:00",
  "updated_at": "2025-07-21T13:36:14.937391+00:00",
  "query": "What are the most promising approaches for scaling transformer models to handle extremely long contexts while maintaining computational efficiency?",
  "thinking_mode": "deep",
  "total_permutations": 5040,
  "completed_permutations": 0,
  "current_sequence": "",
  "estimated_completion": null,
  "permutations_per_minute": 0.0,
  "elapsed_hours": 0.0,
  "estimated_hours_remaining": 0.0,
  "memory_usage_mb": 0.0,
  "cpu_usage_percent": 0.0,
  "ftns_tokens_distributed": 0.0,
  "ftns_payments_count": 0,
  "confidence_score": 0.0,
  "papers_analyzed": 0,
  "final_answer_length": 0,
  "citations_count": 0,
  "error_count": 0,
  "last_error": "",
  "recovery_count": 0
}